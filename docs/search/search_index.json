{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kindle - PyTorch no-code model builder Kindle is an easy model build package for PyTorch . Building a deep learning model became so simple that almost all model can be made by copy and paste from other existing model codes. So why code? when we can simply build a model with yaml markup file. Kindle builds a model with no code but yaml file which its method is inspired from YOLOv5 . Installation PyTorch is required prior to install. Please visit PyTorch installation guide to install. You can install kindle by pip. $ pip install kindle","title":"Home"},{"location":"#kindle-pytorch-no-code-model-builder","text":"Kindle is an easy model build package for PyTorch . Building a deep learning model became so simple that almost all model can be made by copy and paste from other existing model codes. So why code? when we can simply build a model with yaml markup file. Kindle builds a model with no code but yaml file which its method is inspired from YOLOv5 .","title":"Kindle - PyTorch no-code model builder"},{"location":"#installation","text":"PyTorch is required prior to install. Please visit PyTorch installation guide to install. You can install kindle by pip. $ pip install kindle","title":"Installation"},{"location":"modules/","text":"Kindle Modules Supported Modules Summary Module Components Arguments Conv Conv -> BatchNorm -> Activation [channel, kernel size, stride, padding, activation] DWConv DWConv -> BatchNorm -> Activation [channel, kernel_size, stride, padding, activation] Bottleneck Expansion ConvBNAct -> ConvBNAct [channel, shortcut, groups, expansion, activation] AvgPool Average pooling [kernel_size, stride, padding] MaxPool Max pooling [kernel_size, stride, padding] GlobalAvgPool Global Average Pooling [] Flatten Flatten [] Concat Concatenation [dimension] Linear Linear [channel, activation] Add Add [] UpSample UpSample []","title":"Modules"},{"location":"modules/#kindle-modules","text":"","title":"Kindle Modules"},{"location":"modules/#supported-modules-summary","text":"Module Components Arguments Conv Conv -> BatchNorm -> Activation [channel, kernel size, stride, padding, activation] DWConv DWConv -> BatchNorm -> Activation [channel, kernel_size, stride, padding, activation] Bottleneck Expansion ConvBNAct -> ConvBNAct [channel, shortcut, groups, expansion, activation] AvgPool Average pooling [kernel_size, stride, padding] MaxPool Max pooling [kernel_size, stride, padding] GlobalAvgPool Global Average Pooling [] Flatten Flatten [] Concat Concatenation [dimension] Linear Linear [channel, activation] Add Add [] UpSample UpSample []","title":"Supported Modules Summary"},{"location":"tutorial/","text":"Tutorial 1. Building a PyTorch model with yaml Kindle builds a PyTorch model with yaml file. Components input_size : (Tuple[int, int]) (Optional) Model input image size(height, width). input_channel : (float) Model input channel size. Note ex) If input_size : [32, 32] and input_channel : 3 are given, input size of the model will be (batch_size, 3, 32, 32). When input_size is not provided, Kindle assumes that the model can take any input size. depth_multiple : (float) Depth multiplication factor. width_multiple : (float) Width multiplication factor. custom_module_paths : (List[str]) (Optional) Custom module python script path list. backbone : (List[ module ]) Model layers. head : (List[ module ]) (Optional) Model head. This section is same width backbone but width_multiplier is not considered which makes head to have fixed channel size. Note backbone and head consist of module list. module : (List[int, int, str, List]) [ from index , repeat , module name , module arguments ] from index : Index number of the input for the module. -1 represents a previous module. Index number of head is continued from backbone . First module in backbone must have -1 from index value which represents input image. repeat : Repeat number of the module. Ex) When Conv module has repeat: 2 , this module will perform Conv operation twice (Input -> Conv -> Conv). module_name : Name of the module. Pre-built modules are descried here . module_arguments : Arguments of the module. Each module takes pre-defined arguments. Pre-built module arguments are descried here . Example input_size : [ 32 , 32 ] input_channel : 3 depth_multiple : 1.0 width_multiple : 1.0 backbone : # [from, repeat, module, args] [ [ -1 , 1 , Conv , [ 8 , 3 , 1 ]], [ -1 , 1 , MaxPool , [ 2 ]], [ -1 , 1 , Conv , [ 16 , 3 , 1 ]], [ -1 , 1 , MaxPool , [ 2 ]], [ -1 , 1 , Flatten , []], [ -1 , 1 , Linear , [ 120 , ReLU ]], [ -1 , 1 , Linear , [ 84 , ReLU ]], ] head : [ [ -1 , 1 , Linear , [ 10 ]] ] 2. Design Custom Module with YAML You can make your own custom module with yaml file. 1. custom_module.yaml args : [ 96 , 32 ] module : # [from, repeat, module, args] [ [ -1 , 1 , Conv , [ arg0 , 1 , 1 ]], [ 0 , 1 , Conv , [ arg1 , 3 , 1 ]], [ 0 , 1 , Conv , [ arg1 , 5 , 1 ]], [ 0 , 1 , Conv , [ arg1 , 7 , 1 ]], [[ 1 , 2 , 3 ], 1 , Concat , [ 1 ]], [[ 0 , 4 ], 1 , Add , []], ] Arguments of yaml module can be defined as arg0, arg1 ... 2. model_with_custom_module.yaml input_size : [ 32 , 32 ] input_channel : 3 depth_multiple : 1.0 width_multiple : 1.0 backbone : [ [ -1 , 1 , Conv , [ 6 , 5 , 1 , 0 ]], [ -1 , 1 , MaxPool , [ 2 ]], [ -1 , 1 , YamlModule , [ \"custom_module.yaml\" , 48 , 16 ]], [ -1 , 1 , MaxPool , [ 2 ]], [ -1 , 1 , Flatten , []], [ -1 , 1 , Linear , [ 120 , ReLU ]], [ -1 , 1 , Linear , [ 84 , ReLU ]], [ -1 , 1 , Linear , [ 10 ]] ] * Note that argument of yaml module can be provided. 3. Build model from kindle import Model model = Model ( \"model_with_custom_module.yaml\" ), verbose = True ) idx | from | n | params | module | arguments | in shape | out shape | --------------------------------------------------------------------------------------------------------------------------------- 0 | -1 | 1 | 616 | Conv | [ 6 , 5 , 1 , 0 ] | [ 3 , 32 , 32 ] | [ 8 , 32 , 32 ] | 1 | -1 | 1 | 0 | MaxPool | [ 2 ] | [ 8 32 32 ] | [ 8 , 16 , 16 ] | 2 | -1 | 1 | 10 ,832 | YamlModule | [ 'custom_module' ] | [ 8 16 16 ] | [ 24 , 16 , 16 ] | 3 | -1 | 1 | 0 | MaxPool | [ 2 ] | [ 24 16 16 ] | [ 24 , 8 , 8 ] | 4 | -1 | 1 | 0 | Flatten | [] | [ 24 8 8 ] | [ 1536 ] | 5 | -1 | 1 | 184 ,440 | Linear | [ 120 , 'ReLU' ] | [ 1536 ] | [ 120 ] | 6 | -1 | 1 | 10 ,164 | Linear | [ 84 , 'ReLU' ] | [ 120 ] | [ 84 ] | 7 | -1 | 1 | 850 | Linear | [ 10 ] | [ 84 ] | [ 10 ] | Model Summary: 36 layers, 206 ,902 parameters, 206 ,902 gradients 3. Design Custom Module from Source You can make your own custom module from the source. 1. custom_module_model.yaml input_size : [ 32 , 32 ] input_channel : 3 depth_multiple : 1.0 width_multiple : 1.0 custom_module_paths : [ \"tests.test_custom_module\" ] # Paths to the custom modules of the source backbone : # [from, repeat, module, args] [ [ -1 , 1 , MyConv , [ 6 , 5 , 3 ]], [ -1 , 1 , MaxPool , [ 2 ]], [ -1 , 1 , MyConv , [ 16 , 3 , 5 , SiLU ]], [ -1 , 1 , MaxPool , [ 2 ]], [ -1 , 1 , Flatten , []], [ -1 , 1 , Linear , [ 120 , ReLU ]], [ -1 , 1 , Linear , [ 84 , ReLU ]], [ -1 , 1 , Linear , [ 10 ]] ] 2. Write PyTorch module and ModuleGenerator tests/test_custom_module.py from typing import List , Union import numpy as np import torch from torch import nn from kindle.generator import GeneratorAbstract from kindle.torch_utils import Activation , autopad class MyConv ( nn . Module ): def __init__ ( self , in_channels : int , out_channels : int , kernel_size : int , n : int , activation : Union [ str , None ] = \"ReLU\" , ) -> None : super () . __init__ () convs = [] for i in range ( n ): convs . append ( nn . Conv2d ( in_channels , in_channels if ( i + 1 ) != n else out_channels , kernel_size , padding = autopad ( kernel_size ), bias = False , ) ) self . convs = nn . Sequential ( * convs ) self . batch_norm = nn . BatchNorm2d ( out_channels ) self . activation = Activation ( activation )() def forward ( self , x : torch . Tensor ) -> torch . Tensor : return self . activation ( self . batch_norm ( self . convs ( x ))) class MyConvGenerator ( GeneratorAbstract ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) @property def out_channel ( self ) -> int : return self . _get_divisible_channel ( self . args [ 0 ] * self . width_multiply ) @property def in_channel ( self ) -> int : if isinstance ( self . from_idx , list ): raise Exception ( \"from_idx can not be a list.\" ) return self . in_channels [ self . from_idx ] @torch . no_grad () def compute_out_shape ( self , size : np . ndarray , repeat : int = 1 ) -> List [ int ]: module = self ( repeat = repeat ) module . eval () module_out = module ( torch . zeros ([ 1 , * list ( size )])) return list ( module_out . shape [ - 3 :]) def __call__ ( self , repeat : int = 1 ) -> nn . Module : args = [ self . in_channel , self . out_channel , * self . args [ 1 :]] if repeat > 1 : module = [ MyConv ( * args ) for _ in range ( repeat )] else : module = MyConv ( * args ) return self . _get_module ( module ) 3. Build a model from kindle import Model model = Model ( \"custom_module_model.yaml\" ), verbose = True ) idx | from | n | params | module | arguments | in shape | out shape | --------------------------------------------------------------------------------------------------------------------------------- 0 | -1 | 1 | 1 ,066 | MyConv | [ 6 , 5 , 3 ] | [ 3 , 32 , 32 ] | [ 8 , 32 , 32 ] | 1 | -1 | 1 | 0 | MaxPool | [ 2 ] | [ 8 32 32 ] | [ 8 , 16 , 16 ] | 2 | -1 | 1 | 3 ,488 | MyConv | [ 16 , 3 , 5 , 'SiLU' ] | [ 8 16 16 ] | [ 16 , 16 , 16 ] | 3 | -1 | 1 | 0 | MaxPool | [ 2 ] | [ 16 16 16 ] | [ 16 , 8 , 8 ] | 4 | -1 | 1 | 0 | Flatten | [] | [ 16 8 8 ] | [ 1024 ] | 5 | -1 | 1 | 123 ,000 | Linear | [ 120 , 'ReLU' ] | [ 1024 ] | [ 120 ] | 6 | -1 | 1 | 10 ,164 | Linear | [ 84 , 'ReLU' ] | [ 120 ] | [ 84 ] | 7 | -1 | 1 | 850 | Linear | [ 10 ] | [ 84 ] | [ 10 ] | Model Summary: 29 layers, 138 ,568 parameters, 138 ,568 gradients","title":"Tutorial"},{"location":"tutorial/#tutorial","text":"","title":"Tutorial"},{"location":"tutorial/#1-building-a-pytorch-model-with-yaml","text":"Kindle builds a PyTorch model with yaml file.","title":"1. Building a PyTorch model with yaml"},{"location":"tutorial/#components","text":"input_size : (Tuple[int, int]) (Optional) Model input image size(height, width). input_channel : (float) Model input channel size. Note ex) If input_size : [32, 32] and input_channel : 3 are given, input size of the model will be (batch_size, 3, 32, 32). When input_size is not provided, Kindle assumes that the model can take any input size. depth_multiple : (float) Depth multiplication factor. width_multiple : (float) Width multiplication factor. custom_module_paths : (List[str]) (Optional) Custom module python script path list. backbone : (List[ module ]) Model layers. head : (List[ module ]) (Optional) Model head. This section is same width backbone but width_multiplier is not considered which makes head to have fixed channel size. Note backbone and head consist of module list. module : (List[int, int, str, List]) [ from index , repeat , module name , module arguments ] from index : Index number of the input for the module. -1 represents a previous module. Index number of head is continued from backbone . First module in backbone must have -1 from index value which represents input image. repeat : Repeat number of the module. Ex) When Conv module has repeat: 2 , this module will perform Conv operation twice (Input -> Conv -> Conv). module_name : Name of the module. Pre-built modules are descried here . module_arguments : Arguments of the module. Each module takes pre-defined arguments. Pre-built module arguments are descried here .","title":"Components"},{"location":"tutorial/#example","text":"input_size : [ 32 , 32 ] input_channel : 3 depth_multiple : 1.0 width_multiple : 1.0 backbone : # [from, repeat, module, args] [ [ -1 , 1 , Conv , [ 8 , 3 , 1 ]], [ -1 , 1 , MaxPool , [ 2 ]], [ -1 , 1 , Conv , [ 16 , 3 , 1 ]], [ -1 , 1 , MaxPool , [ 2 ]], [ -1 , 1 , Flatten , []], [ -1 , 1 , Linear , [ 120 , ReLU ]], [ -1 , 1 , Linear , [ 84 , ReLU ]], ] head : [ [ -1 , 1 , Linear , [ 10 ]] ]","title":"Example"},{"location":"tutorial/#2-design-custom-module-with-yaml","text":"You can make your own custom module with yaml file. 1. custom_module.yaml args : [ 96 , 32 ] module : # [from, repeat, module, args] [ [ -1 , 1 , Conv , [ arg0 , 1 , 1 ]], [ 0 , 1 , Conv , [ arg1 , 3 , 1 ]], [ 0 , 1 , Conv , [ arg1 , 5 , 1 ]], [ 0 , 1 , Conv , [ arg1 , 7 , 1 ]], [[ 1 , 2 , 3 ], 1 , Concat , [ 1 ]], [[ 0 , 4 ], 1 , Add , []], ] Arguments of yaml module can be defined as arg0, arg1 ... 2. model_with_custom_module.yaml input_size : [ 32 , 32 ] input_channel : 3 depth_multiple : 1.0 width_multiple : 1.0 backbone : [ [ -1 , 1 , Conv , [ 6 , 5 , 1 , 0 ]], [ -1 , 1 , MaxPool , [ 2 ]], [ -1 , 1 , YamlModule , [ \"custom_module.yaml\" , 48 , 16 ]], [ -1 , 1 , MaxPool , [ 2 ]], [ -1 , 1 , Flatten , []], [ -1 , 1 , Linear , [ 120 , ReLU ]], [ -1 , 1 , Linear , [ 84 , ReLU ]], [ -1 , 1 , Linear , [ 10 ]] ] * Note that argument of yaml module can be provided. 3. Build model from kindle import Model model = Model ( \"model_with_custom_module.yaml\" ), verbose = True ) idx | from | n | params | module | arguments | in shape | out shape | --------------------------------------------------------------------------------------------------------------------------------- 0 | -1 | 1 | 616 | Conv | [ 6 , 5 , 1 , 0 ] | [ 3 , 32 , 32 ] | [ 8 , 32 , 32 ] | 1 | -1 | 1 | 0 | MaxPool | [ 2 ] | [ 8 32 32 ] | [ 8 , 16 , 16 ] | 2 | -1 | 1 | 10 ,832 | YamlModule | [ 'custom_module' ] | [ 8 16 16 ] | [ 24 , 16 , 16 ] | 3 | -1 | 1 | 0 | MaxPool | [ 2 ] | [ 24 16 16 ] | [ 24 , 8 , 8 ] | 4 | -1 | 1 | 0 | Flatten | [] | [ 24 8 8 ] | [ 1536 ] | 5 | -1 | 1 | 184 ,440 | Linear | [ 120 , 'ReLU' ] | [ 1536 ] | [ 120 ] | 6 | -1 | 1 | 10 ,164 | Linear | [ 84 , 'ReLU' ] | [ 120 ] | [ 84 ] | 7 | -1 | 1 | 850 | Linear | [ 10 ] | [ 84 ] | [ 10 ] | Model Summary: 36 layers, 206 ,902 parameters, 206 ,902 gradients","title":"2. Design Custom Module with YAML"},{"location":"tutorial/#3-design-custom-module-from-source","text":"You can make your own custom module from the source. 1. custom_module_model.yaml input_size : [ 32 , 32 ] input_channel : 3 depth_multiple : 1.0 width_multiple : 1.0 custom_module_paths : [ \"tests.test_custom_module\" ] # Paths to the custom modules of the source backbone : # [from, repeat, module, args] [ [ -1 , 1 , MyConv , [ 6 , 5 , 3 ]], [ -1 , 1 , MaxPool , [ 2 ]], [ -1 , 1 , MyConv , [ 16 , 3 , 5 , SiLU ]], [ -1 , 1 , MaxPool , [ 2 ]], [ -1 , 1 , Flatten , []], [ -1 , 1 , Linear , [ 120 , ReLU ]], [ -1 , 1 , Linear , [ 84 , ReLU ]], [ -1 , 1 , Linear , [ 10 ]] ] 2. Write PyTorch module and ModuleGenerator tests/test_custom_module.py from typing import List , Union import numpy as np import torch from torch import nn from kindle.generator import GeneratorAbstract from kindle.torch_utils import Activation , autopad class MyConv ( nn . Module ): def __init__ ( self , in_channels : int , out_channels : int , kernel_size : int , n : int , activation : Union [ str , None ] = \"ReLU\" , ) -> None : super () . __init__ () convs = [] for i in range ( n ): convs . append ( nn . Conv2d ( in_channels , in_channels if ( i + 1 ) != n else out_channels , kernel_size , padding = autopad ( kernel_size ), bias = False , ) ) self . convs = nn . Sequential ( * convs ) self . batch_norm = nn . BatchNorm2d ( out_channels ) self . activation = Activation ( activation )() def forward ( self , x : torch . Tensor ) -> torch . Tensor : return self . activation ( self . batch_norm ( self . convs ( x ))) class MyConvGenerator ( GeneratorAbstract ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) @property def out_channel ( self ) -> int : return self . _get_divisible_channel ( self . args [ 0 ] * self . width_multiply ) @property def in_channel ( self ) -> int : if isinstance ( self . from_idx , list ): raise Exception ( \"from_idx can not be a list.\" ) return self . in_channels [ self . from_idx ] @torch . no_grad () def compute_out_shape ( self , size : np . ndarray , repeat : int = 1 ) -> List [ int ]: module = self ( repeat = repeat ) module . eval () module_out = module ( torch . zeros ([ 1 , * list ( size )])) return list ( module_out . shape [ - 3 :]) def __call__ ( self , repeat : int = 1 ) -> nn . Module : args = [ self . in_channel , self . out_channel , * self . args [ 1 :]] if repeat > 1 : module = [ MyConv ( * args ) for _ in range ( repeat )] else : module = MyConv ( * args ) return self . _get_module ( module ) 3. Build a model from kindle import Model model = Model ( \"custom_module_model.yaml\" ), verbose = True ) idx | from | n | params | module | arguments | in shape | out shape | --------------------------------------------------------------------------------------------------------------------------------- 0 | -1 | 1 | 1 ,066 | MyConv | [ 6 , 5 , 3 ] | [ 3 , 32 , 32 ] | [ 8 , 32 , 32 ] | 1 | -1 | 1 | 0 | MaxPool | [ 2 ] | [ 8 32 32 ] | [ 8 , 16 , 16 ] | 2 | -1 | 1 | 3 ,488 | MyConv | [ 16 , 3 , 5 , 'SiLU' ] | [ 8 16 16 ] | [ 16 , 16 , 16 ] | 3 | -1 | 1 | 0 | MaxPool | [ 2 ] | [ 16 16 16 ] | [ 16 , 8 , 8 ] | 4 | -1 | 1 | 0 | Flatten | [] | [ 16 8 8 ] | [ 1024 ] | 5 | -1 | 1 | 123 ,000 | Linear | [ 120 , 'ReLU' ] | [ 1024 ] | [ 120 ] | 6 | -1 | 1 | 10 ,164 | Linear | [ 84 , 'ReLU' ] | [ 120 ] | [ 84 ] | 7 | -1 | 1 | 850 | Linear | [ 10 ] | [ 84 ] | [ 10 ] | Model Summary: 29 layers, 138 ,568 parameters, 138 ,568 gradients","title":"3. Design Custom Module from Source"},{"location":"usages/","text":"Usages AutoML with Optuna Kindle offers the easiest way to build your own deep learning architecture. Beyond building a model, AutoML became easier with Kindle and Optuna or other optimization frameworks. Example code import torch from torch import nn , optim from torchvision import datasets , transforms from torch.utils.data.sampler import SubsetRandomSampler from torch.utils.data import DataLoader from kindle import Model , TorchTrainer import optuna if __name__ == \"__main__\" : device = torch . device ( \"cuda:0\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) preprocess = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))] ) train_dataset = datasets . CIFAR10 ( \"./data/cifar10\" , train = True , download = True , transform = preprocess ) test_dataset = datasets . CIFAR10 ( \"./data/cifar10\" , train = False , download = True , transform = preprocess ) subset_sampler = SubsetRandomSampler ( np . arange ( 0 , len ( train_dataset ), 2 )) def objective ( trial : optuna . Trial ): model_cfg = { \"input_size\" : [ 32 , 32 ], \"input_channel\" : 3 , \"depth_multiple\" : 1.0 , \"width_multiple\" : 1.0 } conv_type = trial . suggest_categorical ( \"conv_type\" , [ \"Conv\" , \"DWConv\" ]) kernel_size = trial . suggest_int ( \"kernel_size\" , 3 , 7 , step = 2 ) n_channel_01 = trial . suggest_int ( \"n_channel_01\" , 8 , 64 , step = 8 ) n_channel_02 = trial . suggest_int ( \"n_channel_02\" , 8 , 128 , step = 8 ) linear_activation = trial . suggest_categorical ( \"linear_activation\" , [ \"ReLU\" , \"SiLU\" ]) n_channel_03 = trial . suggest_int ( \"n_channel_03\" , 64 , 256 , step = 8 ) n_channel_04 = trial . suggest_int ( \"n_channel_04\" , 32 , 128 , step = 8 ) n_repeat = trial . suggest_int ( \"n_repeat\" , 1 , 3 ) backbone = [ [ - 1 , n_repeat , conv_type , [ n_channel_01 , kernel_size , 1 ]], [ - 1 , 1 , \"MaxPool\" , [ 2 ]], [ - 1 , n_repeat , conv_type , [ int ( n_channel_02 ), kernel_size , 1 ]], [ - 1 , 1 , \"MaxPool\" , [ 2 ]], [ - 1 , 1 , \"Flatten\" , []], [ - 1 , 1 , \"Linear\" , [ n_channel_03 , linear_activation ]], [ - 1 , 1 , \"Linear\" , [ n_channel_04 , linear_activation ]], [ - 1 , 1 , \"Linear\" , [ 10 ]], ] model_cfg . update ({ \"backbone\" : backbone }) model = Model ( model_cfg , verbose = True ) batch_size = trial . suggest_int ( \"batch_size\" , 8 , 256 ) epochs = trial . suggest_int ( \"epochs\" , 5 , 20 ) train_loader = DataLoader ( train_dataset , batch_size = batch_size , sampler = subset_sampler ) test_loader = DataLoader ( test_dataset , batch_size = batch_size ) criterion = nn . CrossEntropyLoss () optimizer = optim . Adam ( model . parameters ()) trainer = TorchTrainer ( model , criterion , optimizer , device = device ) trainer . train ( train_loader , n_epoch = epochs , test_dataloader = test_loader ) test_loss , test_accuracy = trainer . test ( test_loader ) return test_loss study = optuna . create_study ( study_name = \"Sample AutoML\" , direction = \"minimize\" ) study . optimize ( objective )","title":"Usages"},{"location":"usages/#usages","text":"","title":"Usages"},{"location":"usages/#automl-with-optuna","text":"Kindle offers the easiest way to build your own deep learning architecture. Beyond building a model, AutoML became easier with Kindle and Optuna or other optimization frameworks.","title":"AutoML with Optuna"},{"location":"usages/#example-code","text":"import torch from torch import nn , optim from torchvision import datasets , transforms from torch.utils.data.sampler import SubsetRandomSampler from torch.utils.data import DataLoader from kindle import Model , TorchTrainer import optuna if __name__ == \"__main__\" : device = torch . device ( \"cuda:0\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) preprocess = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))] ) train_dataset = datasets . CIFAR10 ( \"./data/cifar10\" , train = True , download = True , transform = preprocess ) test_dataset = datasets . CIFAR10 ( \"./data/cifar10\" , train = False , download = True , transform = preprocess ) subset_sampler = SubsetRandomSampler ( np . arange ( 0 , len ( train_dataset ), 2 )) def objective ( trial : optuna . Trial ): model_cfg = { \"input_size\" : [ 32 , 32 ], \"input_channel\" : 3 , \"depth_multiple\" : 1.0 , \"width_multiple\" : 1.0 } conv_type = trial . suggest_categorical ( \"conv_type\" , [ \"Conv\" , \"DWConv\" ]) kernel_size = trial . suggest_int ( \"kernel_size\" , 3 , 7 , step = 2 ) n_channel_01 = trial . suggest_int ( \"n_channel_01\" , 8 , 64 , step = 8 ) n_channel_02 = trial . suggest_int ( \"n_channel_02\" , 8 , 128 , step = 8 ) linear_activation = trial . suggest_categorical ( \"linear_activation\" , [ \"ReLU\" , \"SiLU\" ]) n_channel_03 = trial . suggest_int ( \"n_channel_03\" , 64 , 256 , step = 8 ) n_channel_04 = trial . suggest_int ( \"n_channel_04\" , 32 , 128 , step = 8 ) n_repeat = trial . suggest_int ( \"n_repeat\" , 1 , 3 ) backbone = [ [ - 1 , n_repeat , conv_type , [ n_channel_01 , kernel_size , 1 ]], [ - 1 , 1 , \"MaxPool\" , [ 2 ]], [ - 1 , n_repeat , conv_type , [ int ( n_channel_02 ), kernel_size , 1 ]], [ - 1 , 1 , \"MaxPool\" , [ 2 ]], [ - 1 , 1 , \"Flatten\" , []], [ - 1 , 1 , \"Linear\" , [ n_channel_03 , linear_activation ]], [ - 1 , 1 , \"Linear\" , [ n_channel_04 , linear_activation ]], [ - 1 , 1 , \"Linear\" , [ 10 ]], ] model_cfg . update ({ \"backbone\" : backbone }) model = Model ( model_cfg , verbose = True ) batch_size = trial . suggest_int ( \"batch_size\" , 8 , 256 ) epochs = trial . suggest_int ( \"epochs\" , 5 , 20 ) train_loader = DataLoader ( train_dataset , batch_size = batch_size , sampler = subset_sampler ) test_loader = DataLoader ( test_dataset , batch_size = batch_size ) criterion = nn . CrossEntropyLoss () optimizer = optim . Adam ( model . parameters ()) trainer = TorchTrainer ( model , criterion , optimizer , device = device ) trainer . train ( train_loader , n_epoch = epochs , test_dataloader = test_loader ) test_loss , test_accuracy = trainer . test ( test_loader ) return test_loss study = optuna . create_study ( study_name = \"Sample AutoML\" , direction = \"minimize\" ) study . optimize ( objective )","title":"Example code"},{"location":"api/kindle.generator.add/","text":"module kindle.generator . add </> Module Description. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes AddGenerator \u2014 Add module generator. </> class kindle.generator.add . AddGenerator ( *args , **kwargs ) </> Bases kindle.generator.base_generator.GeneratorAbstract Add module generator. Attributes name (str) \u2014 Module name. </> Methods __call__ ( repeat ) (Module) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute output shape when {size} is given. </> method compute_out_shape ( size , repeat=1 ) \u2192 list of int </> Compute output shape when {size} is given. Args: input size to compute output shape. method __call__ ( repeat=1 ) \u2192 Module </> Returns nn.Module component.","title":"kindle.generator.add"},{"location":"api/kindle.generator.add/#kindlegeneratoradd","text":"</> Module Description. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes AddGenerator \u2014 Add module generator. </> class","title":"kindle.generator.add"},{"location":"api/kindle.generator.add/#kindlegeneratoraddaddgenerator","text":"</> Bases kindle.generator.base_generator.GeneratorAbstract Add module generator. Attributes name (str) \u2014 Module name. </> Methods __call__ ( repeat ) (Module) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute output shape when {size} is given. </> method","title":"kindle.generator.add.AddGenerator"},{"location":"api/kindle.generator.add/#kindlegeneratoraddaddgeneratorcompute_out_shape","text":"</> Compute output shape when {size} is given. Args: input size to compute output shape. method","title":"kindle.generator.add.AddGenerator.compute_out_shape"},{"location":"api/kindle.generator.add/#kindlegeneratoraddaddgeneratorcall","text":"</> Returns nn.Module component.","title":"kindle.generator.add.AddGenerator.call"},{"location":"api/kindle.generator.base_generator/","text":"module kindle.generator . base_generator </> Base Module Generator. This module is responsible for GeneratorAbstract and ModuleGenerator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes GeneratorAbstract ( *args , from_idx , in_channels , width_multiply ) \u2014 Abstract Module Generator. </> ModuleGenerator \u2014 Module generator class. </> abstract class kindle.generator.base_generator . GeneratorAbstract ( *args , from_idx=-1 , in_channels=(0,) , width_multiply=1.0 ) </> Abstract Module Generator. Attributes in_channel (int) \u2014 In channel of the module. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Out channel of the module. </> Methods __call__ ( repeat ) (Module) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute output shape when {size} is given. </> abstract method compute_out_shape ( size , repeat=1 ) \u2192 list of int </> Compute output shape when {size} is given. Args: input size to compute output shape. abstract method __call__ ( repeat=1 ) \u2192 Module </> Returns nn.Module component. class kindle.generator.base_generator . ModuleGenerator ( module_name , custom_module_paths=None ) </> Module generator class.","title":"kindle.generator.base_generator"},{"location":"api/kindle.generator.base_generator/#kindlegeneratorbase_generator","text":"</> Base Module Generator. This module is responsible for GeneratorAbstract and ModuleGenerator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes GeneratorAbstract ( *args , from_idx , in_channels , width_multiply ) \u2014 Abstract Module Generator. </> ModuleGenerator \u2014 Module generator class. </> abstract class","title":"kindle.generator.base_generator"},{"location":"api/kindle.generator.base_generator/#kindlegeneratorbase_generatorgeneratorabstract","text":"</> Abstract Module Generator. Attributes in_channel (int) \u2014 In channel of the module. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Out channel of the module. </> Methods __call__ ( repeat ) (Module) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute output shape when {size} is given. </> abstract method","title":"kindle.generator.base_generator.GeneratorAbstract"},{"location":"api/kindle.generator.base_generator/#kindlegeneratorbase_generatorgeneratorabstractcompute_out_shape","text":"</> Compute output shape when {size} is given. Args: input size to compute output shape. abstract method","title":"kindle.generator.base_generator.GeneratorAbstract.compute_out_shape"},{"location":"api/kindle.generator.base_generator/#kindlegeneratorbase_generatorgeneratorabstractcall","text":"</> Returns nn.Module component. class","title":"kindle.generator.base_generator.GeneratorAbstract.call"},{"location":"api/kindle.generator.base_generator/#kindlegeneratorbase_generatormodulegenerator","text":"</> Module generator class.","title":"kindle.generator.base_generator.ModuleGenerator"},{"location":"api/kindle.generator.bottleneck/","text":"module kindle.generator . bottleneck </> Bottleneck module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes BottleneckGenerator \u2014 Bottleneck block generator. </> class kindle.generator.bottleneck . BottleneckGenerator ( *args , **kwargs ) </> Bases kindle.generator.base_generator.GeneratorAbstract Bottleneck block generator. Attributes base_module (Module) \u2014 Returns module class from kindle.common_modules based on the class name. </> in_channel (int) \u2014 Get in channel size. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Get out channel size. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute output shape. </> method compute_out_shape ( size , repeat=1 ) \u2192 list of int </> Compute output shape. method __call__ ( repeat=1 ) </> Returns nn.Module component.","title":"kindle.generator.bottleneck"},{"location":"api/kindle.generator.bottleneck/#kindlegeneratorbottleneck","text":"</> Bottleneck module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes BottleneckGenerator \u2014 Bottleneck block generator. </> class","title":"kindle.generator.bottleneck"},{"location":"api/kindle.generator.bottleneck/#kindlegeneratorbottleneckbottleneckgenerator","text":"</> Bases kindle.generator.base_generator.GeneratorAbstract Bottleneck block generator. Attributes base_module (Module) \u2014 Returns module class from kindle.common_modules based on the class name. </> in_channel (int) \u2014 Get in channel size. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Get out channel size. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute output shape. </> method","title":"kindle.generator.bottleneck.BottleneckGenerator"},{"location":"api/kindle.generator.bottleneck/#kindlegeneratorbottleneckbottleneckgeneratorcompute_out_shape","text":"</> Compute output shape. method","title":"kindle.generator.bottleneck.BottleneckGenerator.compute_out_shape"},{"location":"api/kindle.generator.bottleneck/#kindlegeneratorbottleneckbottleneckgeneratorcall","text":"</> Returns nn.Module component.","title":"kindle.generator.bottleneck.BottleneckGenerator.call"},{"location":"api/kindle.generator.concat/","text":"module kindle.generator . concat </> Concat module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes ConcatGenerator \u2014 Concatenation module generator. </> class kindle.generator.concat . ConcatGenerator ( *args , **kwargs ) </> Bases kindle.generator.base_generator.GeneratorAbstract Concatenation module generator. Attributes in_channel (int) \u2014 Get in channel size. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Get out channel size. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute out shape. </> method compute_out_shape ( size , repeat=1 ) \u2192 list of int </> Compute out shape. method __call__ ( repeat=1 ) </> Returns nn.Module component.","title":"kindle.generator.concat"},{"location":"api/kindle.generator.concat/#kindlegeneratorconcat","text":"</> Concat module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes ConcatGenerator \u2014 Concatenation module generator. </> class","title":"kindle.generator.concat"},{"location":"api/kindle.generator.concat/#kindlegeneratorconcatconcatgenerator","text":"</> Bases kindle.generator.base_generator.GeneratorAbstract Concatenation module generator. Attributes in_channel (int) \u2014 Get in channel size. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Get out channel size. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute out shape. </> method","title":"kindle.generator.concat.ConcatGenerator"},{"location":"api/kindle.generator.concat/#kindlegeneratorconcatconcatgeneratorcompute_out_shape","text":"</> Compute out shape. method","title":"kindle.generator.concat.ConcatGenerator.compute_out_shape"},{"location":"api/kindle.generator.concat/#kindlegeneratorconcatconcatgeneratorcall","text":"</> Returns nn.Module component.","title":"kindle.generator.concat.ConcatGenerator.call"},{"location":"api/kindle.generator.conv/","text":"module kindle.generator . conv </> Conv module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes ConvGenerator \u2014 Conv2d generator for parsing module. </> class kindle.generator.conv . ConvGenerator ( *args , **kwargs ) </> Bases kindle.generator.base_generator.GeneratorAbstract Conv2d generator for parsing module. Attributes base_module (Module) \u2014 Returns module class from kindle.common_modules based on the class name. </> in_channel (int) \u2014 Get in channel size. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Get out channel size. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> method __call__ ( repeat=1 ) </> Returns nn.Module component.","title":"kindle.generator.conv"},{"location":"api/kindle.generator.conv/#kindlegeneratorconv","text":"</> Conv module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes ConvGenerator \u2014 Conv2d generator for parsing module. </> class","title":"kindle.generator.conv"},{"location":"api/kindle.generator.conv/#kindlegeneratorconvconvgenerator","text":"</> Bases kindle.generator.base_generator.GeneratorAbstract Conv2d generator for parsing module. Attributes base_module (Module) \u2014 Returns module class from kindle.common_modules based on the class name. </> in_channel (int) \u2014 Get in channel size. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Get out channel size. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> method","title":"kindle.generator.conv.ConvGenerator"},{"location":"api/kindle.generator.conv/#kindlegeneratorconvconvgeneratorcall","text":"</> Returns nn.Module component.","title":"kindle.generator.conv.ConvGenerator.call"},{"location":"api/kindle.generator.custom_yaml_module/","text":"module kindle.generator . custom_yaml_module </> Module Description. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes YamlModuleGenerator \u2014 Custom yaml module generator. </> Functions convert_yaml_args ( data , args , inplace ) (list) \u2014 Convert yaml data with argument value. </> function kindle.generator.custom_yaml_module . convert_yaml_args ( data , args , inplace=True ) </> Convert yaml data with argument value. Parameters data (list) \u2014 list or tuple that might contain string of 'arg0', 'arg1' ... args (list or tuple) \u2014 argument values to replace with 'arg0', 'arg1' ... inplace (bool, optional) \u2014 if False, it will not overwrite value. Returns (list) overwritten values by replacing 'arg0', 'arg1' ... to args[0], args[1] ... class kindle.generator.custom_yaml_module . YamlModuleGenerator ( *args , **kwargs ) </> Bases kindle.generator.base_generator.GeneratorAbstract Custom yaml module generator. Attributes name (str) \u2014 Module name. </> Methods __call__ ( repeat ) (Module) \u2014 Returns nn.Module component. </> method __call__ ( repeat=1 ) \u2192 Module </> Returns nn.Module component.","title":"kindle.generator.custom_yaml_module"},{"location":"api/kindle.generator.custom_yaml_module/#kindlegeneratorcustom_yaml_module","text":"</> Module Description. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes YamlModuleGenerator \u2014 Custom yaml module generator. </> Functions convert_yaml_args ( data , args , inplace ) (list) \u2014 Convert yaml data with argument value. </> function","title":"kindle.generator.custom_yaml_module"},{"location":"api/kindle.generator.custom_yaml_module/#kindlegeneratorcustom_yaml_moduleconvert_yaml_args","text":"</> Convert yaml data with argument value. Parameters data (list) \u2014 list or tuple that might contain string of 'arg0', 'arg1' ... args (list or tuple) \u2014 argument values to replace with 'arg0', 'arg1' ... inplace (bool, optional) \u2014 if False, it will not overwrite value. Returns (list) overwritten values by replacing 'arg0', 'arg1' ... to args[0], args[1] ... class","title":"kindle.generator.custom_yaml_module.convert_yaml_args"},{"location":"api/kindle.generator.custom_yaml_module/#kindlegeneratorcustom_yaml_moduleyamlmodulegenerator","text":"</> Bases kindle.generator.base_generator.GeneratorAbstract Custom yaml module generator. Attributes name (str) \u2014 Module name. </> Methods __call__ ( repeat ) (Module) \u2014 Returns nn.Module component. </> method","title":"kindle.generator.custom_yaml_module.YamlModuleGenerator"},{"location":"api/kindle.generator.custom_yaml_module/#kindlegeneratorcustom_yaml_moduleyamlmodulegeneratorcall","text":"</> Returns nn.Module component.","title":"kindle.generator.custom_yaml_module.YamlModuleGenerator.call"},{"location":"api/kindle.generator.dwconv/","text":"module kindle.generator . dwconv </> DWConv module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes DWConvGenerator \u2014 Depth-wise convolution generator for parsing module. </> class kindle.generator.dwconv . DWConvGenerator ( *args , **kwargs ) </> Bases kindle.generator.base_generator.GeneratorAbstract Depth-wise convolution generator for parsing module. Attributes base_module (Module) \u2014 Returns module class from kindle.common_modules based on the class name. </> in_channel (int) \u2014 Get in channel size. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Get out channel size. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> method __call__ ( repeat=1 ) </> Returns nn.Module component.","title":"kindle.generator.dwconv"},{"location":"api/kindle.generator.dwconv/#kindlegeneratordwconv","text":"</> DWConv module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes DWConvGenerator \u2014 Depth-wise convolution generator for parsing module. </> class","title":"kindle.generator.dwconv"},{"location":"api/kindle.generator.dwconv/#kindlegeneratordwconvdwconvgenerator","text":"</> Bases kindle.generator.base_generator.GeneratorAbstract Depth-wise convolution generator for parsing module. Attributes base_module (Module) \u2014 Returns module class from kindle.common_modules based on the class name. </> in_channel (int) \u2014 Get in channel size. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Get out channel size. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> method","title":"kindle.generator.dwconv.DWConvGenerator"},{"location":"api/kindle.generator.dwconv/#kindlegeneratordwconvdwconvgeneratorcall","text":"</> Returns nn.Module component.","title":"kindle.generator.dwconv.DWConvGenerator.call"},{"location":"api/kindle.generator.flatten/","text":"module kindle.generator . flatten </> Flatten module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes FlattenGenerator \u2014 Flatten module generator. </> class kindle.generator.flatten . FlattenGenerator ( *args , **kwargs ) </> Bases kindle.generator.base_generator.GeneratorAbstract Flatten module generator. Attributes name (str) \u2014 Module name. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute output shape when {size} is given. </> method compute_out_shape ( size , repeat=1 ) \u2192 list of int </> Compute output shape when {size} is given. Args: input size to compute output shape. method __call__ ( repeat=1 ) </> Returns nn.Module component.","title":"kindle.generator.flatten"},{"location":"api/kindle.generator.flatten/#kindlegeneratorflatten","text":"</> Flatten module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes FlattenGenerator \u2014 Flatten module generator. </> class","title":"kindle.generator.flatten"},{"location":"api/kindle.generator.flatten/#kindlegeneratorflattenflattengenerator","text":"</> Bases kindle.generator.base_generator.GeneratorAbstract Flatten module generator. Attributes name (str) \u2014 Module name. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute output shape when {size} is given. </> method","title":"kindle.generator.flatten.FlattenGenerator"},{"location":"api/kindle.generator.flatten/#kindlegeneratorflattenflattengeneratorcompute_out_shape","text":"</> Compute output shape when {size} is given. Args: input size to compute output shape. method","title":"kindle.generator.flatten.FlattenGenerator.compute_out_shape"},{"location":"api/kindle.generator.flatten/#kindlegeneratorflattenflattengeneratorcall","text":"</> Returns nn.Module component.","title":"kindle.generator.flatten.FlattenGenerator.call"},{"location":"api/kindle.generator.linear/","text":"module kindle.generator . linear </> Linear module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes LinearGenerator \u2014 Linear (fully connected) module generator for parsing. </> class kindle.generator.linear . LinearGenerator ( *args , **kwargs ) </> Bases kindle.generator.base_generator.GeneratorAbstract Linear (fully connected) module generator for parsing. Attributes in_channel (int) \u2014 Get in channel size. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Get out channel size. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute output shape. </> method compute_out_shape ( size , repeat=1 ) \u2192 list of int </> Compute output shape. method __call__ ( repeat=1 ) </> Returns nn.Module component.","title":"kindle.generator.linear"},{"location":"api/kindle.generator.linear/#kindlegeneratorlinear","text":"</> Linear module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes LinearGenerator \u2014 Linear (fully connected) module generator for parsing. </> class","title":"kindle.generator.linear"},{"location":"api/kindle.generator.linear/#kindlegeneratorlinearlineargenerator","text":"</> Bases kindle.generator.base_generator.GeneratorAbstract Linear (fully connected) module generator for parsing. Attributes in_channel (int) \u2014 Get in channel size. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Get out channel size. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute output shape. </> method","title":"kindle.generator.linear.LinearGenerator"},{"location":"api/kindle.generator.linear/#kindlegeneratorlinearlineargeneratorcompute_out_shape","text":"</> Compute output shape. method","title":"kindle.generator.linear.LinearGenerator.compute_out_shape"},{"location":"api/kindle.generator.linear/#kindlegeneratorlinearlineargeneratorcall","text":"</> Returns nn.Module component.","title":"kindle.generator.linear.LinearGenerator.call"},{"location":"api/kindle.generator/","text":"package kindle . generator </> PyTorch Module Generator for parsing model yaml file. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com module kindle.generator . dwconv </> DWConv module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes DWConvGenerator \u2014 Depth-wise convolution generator for parsing module. </> module kindle.generator . flatten </> Flatten module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes FlattenGenerator \u2014 Flatten module generator. </> module kindle.generator . add </> Module Description. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes AddGenerator \u2014 Add module generator. </> module kindle.generator . concat </> Concat module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes ConcatGenerator \u2014 Concatenation module generator. </> module kindle.generator . linear </> Linear module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes LinearGenerator \u2014 Linear (fully connected) module generator for parsing. </> module kindle.generator . upsample </> UpSample module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes UpSampleGenerator \u2014 UpSample module generator. </> module kindle.generator . base_generator </> Base Module Generator. This module is responsible for GeneratorAbstract and ModuleGenerator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes GeneratorAbstract ( *args , from_idx , in_channels , width_multiply ) \u2014 Abstract Module Generator. </> ModuleGenerator \u2014 Module generator class. </> module kindle.generator . bottleneck </> Bottleneck module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes BottleneckGenerator \u2014 Bottleneck block generator. </> module kindle.generator . custom_yaml_module </> Module Description. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes YamlModuleGenerator \u2014 Custom yaml module generator. </> Functions convert_yaml_args ( data , args , inplace ) (list) \u2014 Convert yaml data with argument value. </> module kindle.generator . conv </> Conv module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes ConvGenerator \u2014 Conv2d generator for parsing module. </> module kindle.generator . poolings </> MaxPool, AvgPool, and GlobalAvgPool modules generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes MaxPoolGenerator \u2014 Max pooling module generator. </> AvgPoolGenerator \u2014 Average pooling module generator. </> GlobalAvgPoolGenerator \u2014 Global average pooling module generator. </>","title":"kindle.generator"},{"location":"api/kindle.generator/#kindlegenerator","text":"</> PyTorch Module Generator for parsing model yaml file. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com module","title":"kindle.generator"},{"location":"api/kindle.generator/#kindlegeneratordwconv","text":"</> DWConv module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes DWConvGenerator \u2014 Depth-wise convolution generator for parsing module. </> module","title":"kindle.generator.dwconv"},{"location":"api/kindle.generator/#kindlegeneratorflatten","text":"</> Flatten module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes FlattenGenerator \u2014 Flatten module generator. </> module","title":"kindle.generator.flatten"},{"location":"api/kindle.generator/#kindlegeneratoradd","text":"</> Module Description. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes AddGenerator \u2014 Add module generator. </> module","title":"kindle.generator.add"},{"location":"api/kindle.generator/#kindlegeneratorconcat","text":"</> Concat module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes ConcatGenerator \u2014 Concatenation module generator. </> module","title":"kindle.generator.concat"},{"location":"api/kindle.generator/#kindlegeneratorlinear","text":"</> Linear module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes LinearGenerator \u2014 Linear (fully connected) module generator for parsing. </> module","title":"kindle.generator.linear"},{"location":"api/kindle.generator/#kindlegeneratorupsample","text":"</> UpSample module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes UpSampleGenerator \u2014 UpSample module generator. </> module","title":"kindle.generator.upsample"},{"location":"api/kindle.generator/#kindlegeneratorbase_generator","text":"</> Base Module Generator. This module is responsible for GeneratorAbstract and ModuleGenerator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes GeneratorAbstract ( *args , from_idx , in_channels , width_multiply ) \u2014 Abstract Module Generator. </> ModuleGenerator \u2014 Module generator class. </> module","title":"kindle.generator.base_generator"},{"location":"api/kindle.generator/#kindlegeneratorbottleneck","text":"</> Bottleneck module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes BottleneckGenerator \u2014 Bottleneck block generator. </> module","title":"kindle.generator.bottleneck"},{"location":"api/kindle.generator/#kindlegeneratorcustom_yaml_module","text":"</> Module Description. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes YamlModuleGenerator \u2014 Custom yaml module generator. </> Functions convert_yaml_args ( data , args , inplace ) (list) \u2014 Convert yaml data with argument value. </> module","title":"kindle.generator.custom_yaml_module"},{"location":"api/kindle.generator/#kindlegeneratorconv","text":"</> Conv module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes ConvGenerator \u2014 Conv2d generator for parsing module. </> module","title":"kindle.generator.conv"},{"location":"api/kindle.generator/#kindlegeneratorpoolings","text":"</> MaxPool, AvgPool, and GlobalAvgPool modules generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes MaxPoolGenerator \u2014 Max pooling module generator. </> AvgPoolGenerator \u2014 Average pooling module generator. </> GlobalAvgPoolGenerator \u2014 Global average pooling module generator. </>","title":"kindle.generator.poolings"},{"location":"api/kindle.generator.poolings/","text":"module kindle.generator . poolings </> MaxPool, AvgPool, and GlobalAvgPool modules generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes MaxPoolGenerator \u2014 Max pooling module generator. </> AvgPoolGenerator \u2014 Average pooling module generator. </> GlobalAvgPoolGenerator \u2014 Global average pooling module generator. </> class kindle.generator.poolings . MaxPoolGenerator ( *args , **kwargs ) </> Bases kindle.generator.base_generator.GeneratorAbstract Max pooling module generator. Attributes base_module (Module) \u2014 Base module. </> in_channel (int) \u2014 Get in channel size. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Get out channel size. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute out shape. </> method compute_out_shape ( size , repeat=1 ) \u2192 list of int </> Compute out shape. method __call__ ( repeat=1 ) </> Returns nn.Module component. class kindle.generator.poolings . AvgPoolGenerator ( *args , **kwargs ) </> Bases kindle.generator.poolings.MaxPoolGenerator kindle.generator.base_generator.GeneratorAbstract Average pooling module generator. Attributes base_module (Module) \u2014 Base module. </> in_channel (int) \u2014 Get in channel size. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Get out channel size. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute out shape. </> method compute_out_shape ( size , repeat=1 ) \u2192 list of int </> Compute out shape. method __call__ ( repeat=1 ) </> Returns nn.Module component. class kindle.generator.poolings . GlobalAvgPoolGenerator ( *args , from_idx=-1 , in_channels=(0,) , width_multiply=1.0 ) </> Bases kindle.generator.base_generator.GeneratorAbstract Global average pooling module generator. Attributes in_channel (int) \u2014 Get in channel size. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Get out channel size. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute out shape. </> method compute_out_shape ( size , repeat=1 ) \u2192 list of int </> Compute out shape. method __call__ ( repeat=1 ) </> Returns nn.Module component.","title":"kindle.generator.poolings"},{"location":"api/kindle.generator.poolings/#kindlegeneratorpoolings","text":"</> MaxPool, AvgPool, and GlobalAvgPool modules generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes MaxPoolGenerator \u2014 Max pooling module generator. </> AvgPoolGenerator \u2014 Average pooling module generator. </> GlobalAvgPoolGenerator \u2014 Global average pooling module generator. </> class","title":"kindle.generator.poolings"},{"location":"api/kindle.generator.poolings/#kindlegeneratorpoolingsmaxpoolgenerator","text":"</> Bases kindle.generator.base_generator.GeneratorAbstract Max pooling module generator. Attributes base_module (Module) \u2014 Base module. </> in_channel (int) \u2014 Get in channel size. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Get out channel size. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute out shape. </> method","title":"kindle.generator.poolings.MaxPoolGenerator"},{"location":"api/kindle.generator.poolings/#kindlegeneratorpoolingsmaxpoolgeneratorcompute_out_shape","text":"</> Compute out shape. method","title":"kindle.generator.poolings.MaxPoolGenerator.compute_out_shape"},{"location":"api/kindle.generator.poolings/#kindlegeneratorpoolingsmaxpoolgeneratorcall","text":"</> Returns nn.Module component. class","title":"kindle.generator.poolings.MaxPoolGenerator.call"},{"location":"api/kindle.generator.poolings/#kindlegeneratorpoolingsavgpoolgenerator","text":"</> Bases kindle.generator.poolings.MaxPoolGenerator kindle.generator.base_generator.GeneratorAbstract Average pooling module generator. Attributes base_module (Module) \u2014 Base module. </> in_channel (int) \u2014 Get in channel size. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Get out channel size. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute out shape. </> method","title":"kindle.generator.poolings.AvgPoolGenerator"},{"location":"api/kindle.generator.poolings/#kindlegeneratorpoolingsmaxpoolgeneratorcompute_out_shape_1","text":"</> Compute out shape. method","title":"kindle.generator.poolings.MaxPoolGenerator.compute_out_shape"},{"location":"api/kindle.generator.poolings/#kindlegeneratorpoolingsmaxpoolgeneratorcall_1","text":"</> Returns nn.Module component. class","title":"kindle.generator.poolings.MaxPoolGenerator.call"},{"location":"api/kindle.generator.poolings/#kindlegeneratorpoolingsglobalavgpoolgenerator","text":"</> Bases kindle.generator.base_generator.GeneratorAbstract Global average pooling module generator. Attributes in_channel (int) \u2014 Get in channel size. </> name (str) \u2014 Module name. </> out_channel (int) \u2014 Get out channel size. </> Methods __call__ ( repeat ) \u2014 Returns nn.Module component. </> compute_out_shape ( size , repeat ) (list of int) \u2014 Compute out shape. </> method","title":"kindle.generator.poolings.GlobalAvgPoolGenerator"},{"location":"api/kindle.generator.poolings/#kindlegeneratorpoolingsglobalavgpoolgeneratorcompute_out_shape","text":"</> Compute out shape. method","title":"kindle.generator.poolings.GlobalAvgPoolGenerator.compute_out_shape"},{"location":"api/kindle.generator.poolings/#kindlegeneratorpoolingsglobalavgpoolgeneratorcall","text":"</> Returns nn.Module component.","title":"kindle.generator.poolings.GlobalAvgPoolGenerator.call"},{"location":"api/kindle.generator.upsample/","text":"module kindle.generator . upsample </> UpSample module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes UpSampleGenerator \u2014 UpSample module generator. </> class kindle.generator.upsample . UpSampleGenerator ( *args , **kwargs ) </> Bases kindle.generator.base_generator.GeneratorAbstract UpSample module generator. Attributes name (str) \u2014 Module name. </> Methods __call__ ( repeat ) (Module) \u2014 Returns nn.Module component. </> method __call__ ( repeat=1 ) \u2192 Module </> Returns nn.Module component.","title":"kindle.generator.upsample"},{"location":"api/kindle.generator.upsample/#kindlegeneratorupsample","text":"</> UpSample module generator. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes UpSampleGenerator \u2014 UpSample module generator. </> class","title":"kindle.generator.upsample"},{"location":"api/kindle.generator.upsample/#kindlegeneratorupsampleupsamplegenerator","text":"</> Bases kindle.generator.base_generator.GeneratorAbstract UpSample module generator. Attributes name (str) \u2014 Module name. </> Methods __call__ ( repeat ) (Module) \u2014 Returns nn.Module component. </> method","title":"kindle.generator.upsample.UpSampleGenerator"},{"location":"api/kindle.generator.upsample/#kindlegeneratorupsampleupsamplegeneratorcall","text":"</> Returns nn.Module component.","title":"kindle.generator.upsample.UpSampleGenerator.call"},{"location":"api/kindle/","text":"package kindle </> Kindle is an easy model build package for PyTorch. Building a deep learning model became so simple that almost all model can be made by copy and paste from other existing model codes. So why code? when we can simply build a model with yaml markup file. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com URL: https://github.com/JeiKeiLim/kindle module kindle . model </> Kindle Model parser and model. This module parses model configuration yaml file and generates PyTorch model accordingly. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Model \u2014 PyTorch model class. </> ModelParser \u2014 Generate PyTorch model from the model yaml file. </> module kindle . trainer </> PyTorch trainer module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes TorchTrainer \u2014 Pytorch Trainer. </> module kindle . torch_utils </> Common utility functions. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Activation \u2014 Convert string activation name to the activation class. </> Functions autopad ( kernel_size , padding ) (Union(int, list of int)) \u2014 Auto padding calculation for pad='same' in TensorFlow. </> count_model_params ( model ) (int) \u2014 Count model's parameters. </> make_divisible ( n_channel , divisor ) (int) \u2014 Convert {n_channel} to divisible by {divisor} </> model_info ( model , verbose ) \u2014 Print out model info. </> split_dataset_index ( n_data , split_ratio ) (SubsetRandomSampler, SubsetRandomSampler) \u2014 Split dataset indices with split_ratio. </> package kindle . generator </> PyTorch Module Generator for parsing model yaml file. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com package kindle . modules </> PyTorch Modules.","title":"kindle"},{"location":"api/kindle/#kindle","text":"</> Kindle is an easy model build package for PyTorch. Building a deep learning model became so simple that almost all model can be made by copy and paste from other existing model codes. So why code? when we can simply build a model with yaml markup file. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com URL: https://github.com/JeiKeiLim/kindle module","title":"kindle"},{"location":"api/kindle/#kindlemodel","text":"</> Kindle Model parser and model. This module parses model configuration yaml file and generates PyTorch model accordingly. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Model \u2014 PyTorch model class. </> ModelParser \u2014 Generate PyTorch model from the model yaml file. </> module","title":"kindle.model"},{"location":"api/kindle/#kindletrainer","text":"</> PyTorch trainer module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes TorchTrainer \u2014 Pytorch Trainer. </> module","title":"kindle.trainer"},{"location":"api/kindle/#kindletorch_utils","text":"</> Common utility functions. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Activation \u2014 Convert string activation name to the activation class. </> Functions autopad ( kernel_size , padding ) (Union(int, list of int)) \u2014 Auto padding calculation for pad='same' in TensorFlow. </> count_model_params ( model ) (int) \u2014 Count model's parameters. </> make_divisible ( n_channel , divisor ) (int) \u2014 Convert {n_channel} to divisible by {divisor} </> model_info ( model , verbose ) \u2014 Print out model info. </> split_dataset_index ( n_data , split_ratio ) (SubsetRandomSampler, SubsetRandomSampler) \u2014 Split dataset indices with split_ratio. </> package","title":"kindle.torch_utils"},{"location":"api/kindle/#kindlegenerator","text":"</> PyTorch Module Generator for parsing model yaml file. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com package","title":"kindle.generator"},{"location":"api/kindle/#kindlemodules","text":"</> PyTorch Modules.","title":"kindle.modules"},{"location":"api/kindle.model/","text":"module kindle . model </> Kindle Model parser and model. This module parses model configuration yaml file and generates PyTorch model accordingly. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Model \u2014 PyTorch model class. </> ModelParser \u2014 Generate PyTorch model from the model yaml file. </> class kindle.model . Model ( cfg , verbose=False ) </> Bases torch.nn.modules.module.Module PyTorch model class. Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> bfloat16 ( ) (Module) \u2014 Casts all floating point parameters and buffers to bfloat16 datatype. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) (str) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( x ) (Tensor) \u2014 Forward the model. </> forward_once ( x ) (Tensor) \u2014 Forward one time only. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> register_backward_hook ( hook ) (RemovableHandle) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor , persistent ) \u2014 Adds a buffer to the module. </> register_forward_hook ( hook ) (RemovableHandle) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) (RemovableHandle) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> zero_grad ( set_to_none ) \u2014 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. </> method register_buffer ( name , tensor , persistent=True ) </> Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. persistent (bool) \u2014 whether the buffer is part of this module's :attr: state_dict . Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method register_parameter ( name , param ) </> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method add_module ( name , module ) </> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method apply ( fn ) </> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Returns (Module) self Example:: >>> @torch . no_grad () >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method cuda ( device=None ) </> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method cpu ( ) </> Moves all model parameters and buffers to the CPU. Returns (Module) self method type ( dst_type ) </> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method float ( ) </> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method double ( ) </> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method half ( ) </> Casts all floating point parameters and buffers to half datatype. Returns (Module) self method bfloat16 ( ) </> Casts all floating point parameters and buffers to bfloat16 datatype. Returns (Module) self method to ( *args , **kwargs ) </> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Returns (Module) self Example:: >>> linear = nn . Linear ( 2 , 2 ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]]) >>> linear . to ( torch . double ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]], dtype = torch . float64 ) >>> gpu1 = torch . device ( \"cuda:1\" ) >>> linear . to ( gpu1 , dtype = torch . half , non_blocking = True ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 , device = 'cuda:1' ) >>> cpu = torch . device ( \"cpu\" ) >>> linear . to ( cpu ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 ) method register_backward_hook ( hook ) </> Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method register_forward_pre_hook ( hook ) </> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method register_forward_hook ( hook ) </> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method state_dict ( destination=None , prefix='' , keep_vars=False ) </> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method load_state_dict ( state_dict , strict=True ) </> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator parameters ( recurse=True ) </> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param ), param . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator named_parameters ( prefix='' , recurse=True ) </> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator buffers ( recurse=True ) </> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf ), buf . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator named_buffers ( prefix='' , recurse=True ) </> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator children ( ) </> Returns an iterator over immediate children modules. Yields (Module) a child module generator named_children ( ) </> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator modules ( ) </> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator named_modules ( memo=None , prefix='' ) </> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method train ( mode=True ) </> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method eval ( ) </> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method requires_grad_ ( requires_grad=True ) </> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method zero_grad ( set_to_none=False ) </> Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters set_to_none (bool) \u2014 instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. method extra_repr ( ) \u2192 str </> Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. method forward ( x ) \u2192 Tensor </> Forward the model. For the time being, this method will only call self.forward_once. Later, we plan to add Test Time Augment. method forward_once ( x ) \u2192 Tensor </> Forward one time only. class kindle.model . ModelParser ( cfg='./model_configs/show_case.yaml' , verbose=False ) </> Generate PyTorch model from the model yaml file. Methods log ( msg ) \u2014 Log. </> method log ( msg ) </> Log.","title":"kindle.model"},{"location":"api/kindle.model/#kindlemodel","text":"</> Kindle Model parser and model. This module parses model configuration yaml file and generates PyTorch model accordingly. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Model \u2014 PyTorch model class. </> ModelParser \u2014 Generate PyTorch model from the model yaml file. </> class","title":"kindle.model"},{"location":"api/kindle.model/#kindlemodelmodel","text":"</> Bases torch.nn.modules.module.Module PyTorch model class. Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> bfloat16 ( ) (Module) \u2014 Casts all floating point parameters and buffers to bfloat16 datatype. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) (str) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( x ) (Tensor) \u2014 Forward the model. </> forward_once ( x ) (Tensor) \u2014 Forward one time only. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> register_backward_hook ( hook ) (RemovableHandle) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor , persistent ) \u2014 Adds a buffer to the module. </> register_forward_hook ( hook ) (RemovableHandle) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) (RemovableHandle) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> zero_grad ( set_to_none ) \u2014 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. </> method","title":"kindle.model.Model"},{"location":"api/kindle.model/#torchnnmodulesmodulemoduleregister_buffer","text":"</> Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. persistent (bool) \u2014 whether the buffer is part of this module's :attr: state_dict . Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method","title":"torch.nn.modules.module.Module.register_buffer"},{"location":"api/kindle.model/#torchnnmodulesmodulemoduleregister_parameter","text":"</> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method","title":"torch.nn.modules.module.Module.register_parameter"},{"location":"api/kindle.model/#torchnnmodulesmodulemoduleadd_module","text":"</> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method","title":"torch.nn.modules.module.Module.add_module"},{"location":"api/kindle.model/#torchnnmodulesmodulemoduleapply","text":"</> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Returns (Module) self Example:: >>> @torch . no_grad () >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method","title":"torch.nn.modules.module.Module.apply"},{"location":"api/kindle.model/#torchnnmodulesmodulemodulecuda","text":"</> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method","title":"torch.nn.modules.module.Module.cuda"},{"location":"api/kindle.model/#torchnnmodulesmodulemodulecpu","text":"</> Moves all model parameters and buffers to the CPU. Returns (Module) self method","title":"torch.nn.modules.module.Module.cpu"},{"location":"api/kindle.model/#torchnnmodulesmodulemoduletype","text":"</> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method","title":"torch.nn.modules.module.Module.type"},{"location":"api/kindle.model/#torchnnmodulesmodulemodulefloat","text":"</> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.float"},{"location":"api/kindle.model/#torchnnmodulesmodulemoduledouble","text":"</> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.double"},{"location":"api/kindle.model/#torchnnmodulesmodulemodulehalf","text":"</> Casts all floating point parameters and buffers to half datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.half"},{"location":"api/kindle.model/#torchnnmodulesmodulemodulebfloat16","text":"</> Casts all floating point parameters and buffers to bfloat16 datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.bfloat16"},{"location":"api/kindle.model/#torchnnmodulesmodulemoduleto","text":"</> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Returns (Module) self Example:: >>> linear = nn . Linear ( 2 , 2 ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]]) >>> linear . to ( torch . double ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]], dtype = torch . float64 ) >>> gpu1 = torch . device ( \"cuda:1\" ) >>> linear . to ( gpu1 , dtype = torch . half , non_blocking = True ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 , device = 'cuda:1' ) >>> cpu = torch . device ( \"cpu\" ) >>> linear . to ( cpu ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 ) method","title":"torch.nn.modules.module.Module.to"},{"location":"api/kindle.model/#torchnnmodulesmodulemoduleregister_backward_hook","text":"</> Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_backward_hook"},{"location":"api/kindle.model/#torchnnmodulesmodulemoduleregister_forward_pre_hook","text":"</> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_pre_hook"},{"location":"api/kindle.model/#torchnnmodulesmodulemoduleregister_forward_hook","text":"</> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_hook"},{"location":"api/kindle.model/#torchnnmodulesmodulemodulestate_dict","text":"</> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method","title":"torch.nn.modules.module.Module.state_dict"},{"location":"api/kindle.model/#torchnnmodulesmodulemoduleload_state_dict","text":"</> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator","title":"torch.nn.modules.module.Module.load_state_dict"},{"location":"api/kindle.model/#torchnnmodulesmodulemoduleparameters","text":"</> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param ), param . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.parameters"},{"location":"api/kindle.model/#torchnnmodulesmodulemodulenamed_parameters","text":"</> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator","title":"torch.nn.modules.module.Module.named_parameters"},{"location":"api/kindle.model/#torchnnmodulesmodulemodulebuffers","text":"</> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf ), buf . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.buffers"},{"location":"api/kindle.model/#torchnnmodulesmodulemodulenamed_buffers","text":"</> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator","title":"torch.nn.modules.module.Module.named_buffers"},{"location":"api/kindle.model/#torchnnmodulesmodulemodulechildren","text":"</> Returns an iterator over immediate children modules. Yields (Module) a child module generator","title":"torch.nn.modules.module.Module.children"},{"location":"api/kindle.model/#torchnnmodulesmodulemodulenamed_children","text":"</> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator","title":"torch.nn.modules.module.Module.named_children"},{"location":"api/kindle.model/#torchnnmodulesmodulemodulemodules","text":"</> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator","title":"torch.nn.modules.module.Module.modules"},{"location":"api/kindle.model/#torchnnmodulesmodulemodulenamed_modules","text":"</> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method","title":"torch.nn.modules.module.Module.named_modules"},{"location":"api/kindle.model/#torchnnmodulesmodulemoduletrain","text":"</> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.train"},{"location":"api/kindle.model/#torchnnmodulesmodulemoduleeval","text":"</> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method","title":"torch.nn.modules.module.Module.eval"},{"location":"api/kindle.model/#torchnnmodulesmodulemodulerequires_grad_","text":"</> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.requires_grad_"},{"location":"api/kindle.model/#torchnnmodulesmodulemodulezero_grad","text":"</> Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters set_to_none (bool) \u2014 instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. method","title":"torch.nn.modules.module.Module.zero_grad"},{"location":"api/kindle.model/#torchnnmodulesmodulemoduleextra_repr","text":"</> Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. method","title":"torch.nn.modules.module.Module.extra_repr"},{"location":"api/kindle.model/#kindlemodelmodelforward","text":"</> Forward the model. For the time being, this method will only call self.forward_once. Later, we plan to add Test Time Augment. method","title":"kindle.model.Model.forward"},{"location":"api/kindle.model/#kindlemodelmodelforward_once","text":"</> Forward one time only. class","title":"kindle.model.Model.forward_once"},{"location":"api/kindle.model/#kindlemodelmodelparser","text":"</> Generate PyTorch model from the model yaml file. Methods log ( msg ) \u2014 Log. </> method","title":"kindle.model.ModelParser"},{"location":"api/kindle.model/#kindlemodelmodelparserlog","text":"</> Log.","title":"kindle.model.ModelParser.log"},{"location":"api/kindle.modules.add/","text":"module kindle.modules . add </> Module Description. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Add \u2014 Add module for Kindle. </> class kindle.modules.add . Add ( ) </> Bases torch.nn.modules.module.Module Add module for Kindle. Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> bfloat16 ( ) (Module) \u2014 Casts all floating point parameters and buffers to bfloat16 datatype. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) (str) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( x ) (Tensor) \u2014 Add inputs. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> register_backward_hook ( hook ) (RemovableHandle) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor , persistent ) \u2014 Adds a buffer to the module. </> register_forward_hook ( hook ) (RemovableHandle) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) (RemovableHandle) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> zero_grad ( set_to_none ) \u2014 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. </> method register_buffer ( name , tensor , persistent=True ) </> Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. persistent (bool) \u2014 whether the buffer is part of this module's :attr: state_dict . Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method register_parameter ( name , param ) </> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method add_module ( name , module ) </> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method apply ( fn ) </> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Returns (Module) self Example:: >>> @torch . no_grad () >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method cuda ( device=None ) </> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method cpu ( ) </> Moves all model parameters and buffers to the CPU. Returns (Module) self method type ( dst_type ) </> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method float ( ) </> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method double ( ) </> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method half ( ) </> Casts all floating point parameters and buffers to half datatype. Returns (Module) self method bfloat16 ( ) </> Casts all floating point parameters and buffers to bfloat16 datatype. Returns (Module) self method to ( *args , **kwargs ) </> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Returns (Module) self Example:: >>> linear = nn . Linear ( 2 , 2 ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]]) >>> linear . to ( torch . double ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]], dtype = torch . float64 ) >>> gpu1 = torch . device ( \"cuda:1\" ) >>> linear . to ( gpu1 , dtype = torch . half , non_blocking = True ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 , device = 'cuda:1' ) >>> cpu = torch . device ( \"cpu\" ) >>> linear . to ( cpu ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 ) method register_backward_hook ( hook ) </> Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method register_forward_pre_hook ( hook ) </> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method register_forward_hook ( hook ) </> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method state_dict ( destination=None , prefix='' , keep_vars=False ) </> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method load_state_dict ( state_dict , strict=True ) </> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator parameters ( recurse=True ) </> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param ), param . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator named_parameters ( prefix='' , recurse=True ) </> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator buffers ( recurse=True ) </> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf ), buf . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator named_buffers ( prefix='' , recurse=True ) </> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator children ( ) </> Returns an iterator over immediate children modules. Yields (Module) a child module generator named_children ( ) </> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator modules ( ) </> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator named_modules ( memo=None , prefix='' ) </> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method train ( mode=True ) </> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method eval ( ) </> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method requires_grad_ ( requires_grad=True ) </> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method zero_grad ( set_to_none=False ) </> Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters set_to_none (bool) \u2014 instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. method extra_repr ( ) \u2192 str </> Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. classmethod forward ( x ) </> Add inputs. Parameters x (Union((tensor, ...), list of tensor)) \u2014 list of torch tensors Returns (Tensor) sum of all x's","title":"kindle.modules.add"},{"location":"api/kindle.modules.add/#kindlemodulesadd","text":"</> Module Description. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Add \u2014 Add module for Kindle. </> class","title":"kindle.modules.add"},{"location":"api/kindle.modules.add/#kindlemodulesaddadd","text":"</> Bases torch.nn.modules.module.Module Add module for Kindle. Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> bfloat16 ( ) (Module) \u2014 Casts all floating point parameters and buffers to bfloat16 datatype. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) (str) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( x ) (Tensor) \u2014 Add inputs. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> register_backward_hook ( hook ) (RemovableHandle) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor , persistent ) \u2014 Adds a buffer to the module. </> register_forward_hook ( hook ) (RemovableHandle) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) (RemovableHandle) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> zero_grad ( set_to_none ) \u2014 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. </> method","title":"kindle.modules.add.Add"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemoduleregister_buffer","text":"</> Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. persistent (bool) \u2014 whether the buffer is part of this module's :attr: state_dict . Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method","title":"torch.nn.modules.module.Module.register_buffer"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemoduleregister_parameter","text":"</> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method","title":"torch.nn.modules.module.Module.register_parameter"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemoduleadd_module","text":"</> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method","title":"torch.nn.modules.module.Module.add_module"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemoduleapply","text":"</> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Returns (Module) self Example:: >>> @torch . no_grad () >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method","title":"torch.nn.modules.module.Module.apply"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemodulecuda","text":"</> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method","title":"torch.nn.modules.module.Module.cuda"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemodulecpu","text":"</> Moves all model parameters and buffers to the CPU. Returns (Module) self method","title":"torch.nn.modules.module.Module.cpu"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemoduletype","text":"</> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method","title":"torch.nn.modules.module.Module.type"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemodulefloat","text":"</> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.float"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemoduledouble","text":"</> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.double"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemodulehalf","text":"</> Casts all floating point parameters and buffers to half datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.half"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemodulebfloat16","text":"</> Casts all floating point parameters and buffers to bfloat16 datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.bfloat16"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemoduleto","text":"</> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Returns (Module) self Example:: >>> linear = nn . Linear ( 2 , 2 ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]]) >>> linear . to ( torch . double ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]], dtype = torch . float64 ) >>> gpu1 = torch . device ( \"cuda:1\" ) >>> linear . to ( gpu1 , dtype = torch . half , non_blocking = True ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 , device = 'cuda:1' ) >>> cpu = torch . device ( \"cpu\" ) >>> linear . to ( cpu ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 ) method","title":"torch.nn.modules.module.Module.to"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemoduleregister_backward_hook","text":"</> Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_backward_hook"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemoduleregister_forward_pre_hook","text":"</> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_pre_hook"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemoduleregister_forward_hook","text":"</> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_hook"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemodulestate_dict","text":"</> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method","title":"torch.nn.modules.module.Module.state_dict"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemoduleload_state_dict","text":"</> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator","title":"torch.nn.modules.module.Module.load_state_dict"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemoduleparameters","text":"</> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param ), param . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.parameters"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemodulenamed_parameters","text":"</> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator","title":"torch.nn.modules.module.Module.named_parameters"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemodulebuffers","text":"</> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf ), buf . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.buffers"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemodulenamed_buffers","text":"</> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator","title":"torch.nn.modules.module.Module.named_buffers"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemodulechildren","text":"</> Returns an iterator over immediate children modules. Yields (Module) a child module generator","title":"torch.nn.modules.module.Module.children"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemodulenamed_children","text":"</> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator","title":"torch.nn.modules.module.Module.named_children"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemodulemodules","text":"</> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator","title":"torch.nn.modules.module.Module.modules"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemodulenamed_modules","text":"</> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method","title":"torch.nn.modules.module.Module.named_modules"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemoduletrain","text":"</> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.train"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemoduleeval","text":"</> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method","title":"torch.nn.modules.module.Module.eval"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemodulerequires_grad_","text":"</> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.requires_grad_"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemodulezero_grad","text":"</> Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters set_to_none (bool) \u2014 instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. method","title":"torch.nn.modules.module.Module.zero_grad"},{"location":"api/kindle.modules.add/#torchnnmodulesmodulemoduleextra_repr","text":"</> Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. classmethod","title":"torch.nn.modules.module.Module.extra_repr"},{"location":"api/kindle.modules.add/#kindlemodulesaddaddforward","text":"</> Add inputs. Parameters x (Union((tensor, ...), list of tensor)) \u2014 list of torch tensors Returns (Tensor) sum of all x's","title":"kindle.modules.add.Add.forward"},{"location":"api/kindle.modules.bottleneck/","text":"module kindle.modules . bottleneck </> Bottleneck(ResNet) module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Bottleneck \u2014 Standard bottleneck block. </> class kindle.modules.bottleneck . Bottleneck ( in_channels , out_channels , shortcut=True , groups=1 , expansion=0.5 , activation='ReLU' ) </> Bases torch.nn.modules.module.Module Standard bottleneck block. Arguments: [channel, shortcut, groups, expansion, activation] Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> bfloat16 ( ) (Module) \u2014 Casts all floating point parameters and buffers to bfloat16 datatype. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) (str) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( x ) (Tensor) \u2014 Forward. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> register_backward_hook ( hook ) (RemovableHandle) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor , persistent ) \u2014 Adds a buffer to the module. </> register_forward_hook ( hook ) (RemovableHandle) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) (RemovableHandle) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> zero_grad ( set_to_none ) \u2014 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. </> method register_buffer ( name , tensor , persistent=True ) </> Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. persistent (bool) \u2014 whether the buffer is part of this module's :attr: state_dict . Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method register_parameter ( name , param ) </> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method add_module ( name , module ) </> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method apply ( fn ) </> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Returns (Module) self Example:: >>> @torch . no_grad () >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method cuda ( device=None ) </> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method cpu ( ) </> Moves all model parameters and buffers to the CPU. Returns (Module) self method type ( dst_type ) </> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method float ( ) </> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method double ( ) </> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method half ( ) </> Casts all floating point parameters and buffers to half datatype. Returns (Module) self method bfloat16 ( ) </> Casts all floating point parameters and buffers to bfloat16 datatype. Returns (Module) self method to ( *args , **kwargs ) </> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Returns (Module) self Example:: >>> linear = nn . Linear ( 2 , 2 ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]]) >>> linear . to ( torch . double ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]], dtype = torch . float64 ) >>> gpu1 = torch . device ( \"cuda:1\" ) >>> linear . to ( gpu1 , dtype = torch . half , non_blocking = True ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 , device = 'cuda:1' ) >>> cpu = torch . device ( \"cpu\" ) >>> linear . to ( cpu ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 ) method register_backward_hook ( hook ) </> Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method register_forward_pre_hook ( hook ) </> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method register_forward_hook ( hook ) </> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method state_dict ( destination=None , prefix='' , keep_vars=False ) </> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method load_state_dict ( state_dict , strict=True ) </> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator parameters ( recurse=True ) </> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param ), param . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator named_parameters ( prefix='' , recurse=True ) </> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator buffers ( recurse=True ) </> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf ), buf . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator named_buffers ( prefix='' , recurse=True ) </> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator children ( ) </> Returns an iterator over immediate children modules. Yields (Module) a child module generator named_children ( ) </> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator modules ( ) </> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator named_modules ( memo=None , prefix='' ) </> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method train ( mode=True ) </> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method eval ( ) </> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method requires_grad_ ( requires_grad=True ) </> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method zero_grad ( set_to_none=False ) </> Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters set_to_none (bool) \u2014 instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. method extra_repr ( ) \u2192 str </> Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. method forward ( x ) \u2192 Tensor </> Forward.","title":"kindle.modules.bottleneck"},{"location":"api/kindle.modules.bottleneck/#kindlemodulesbottleneck","text":"</> Bottleneck(ResNet) module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Bottleneck \u2014 Standard bottleneck block. </> class","title":"kindle.modules.bottleneck"},{"location":"api/kindle.modules.bottleneck/#kindlemodulesbottleneckbottleneck","text":"</> Bases torch.nn.modules.module.Module Standard bottleneck block. Arguments: [channel, shortcut, groups, expansion, activation] Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> bfloat16 ( ) (Module) \u2014 Casts all floating point parameters and buffers to bfloat16 datatype. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) (str) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( x ) (Tensor) \u2014 Forward. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> register_backward_hook ( hook ) (RemovableHandle) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor , persistent ) \u2014 Adds a buffer to the module. </> register_forward_hook ( hook ) (RemovableHandle) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) (RemovableHandle) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> zero_grad ( set_to_none ) \u2014 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. </> method","title":"kindle.modules.bottleneck.Bottleneck"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemoduleregister_buffer","text":"</> Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. persistent (bool) \u2014 whether the buffer is part of this module's :attr: state_dict . Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method","title":"torch.nn.modules.module.Module.register_buffer"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemoduleregister_parameter","text":"</> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method","title":"torch.nn.modules.module.Module.register_parameter"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemoduleadd_module","text":"</> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method","title":"torch.nn.modules.module.Module.add_module"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemoduleapply","text":"</> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Returns (Module) self Example:: >>> @torch . no_grad () >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method","title":"torch.nn.modules.module.Module.apply"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemodulecuda","text":"</> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method","title":"torch.nn.modules.module.Module.cuda"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemodulecpu","text":"</> Moves all model parameters and buffers to the CPU. Returns (Module) self method","title":"torch.nn.modules.module.Module.cpu"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemoduletype","text":"</> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method","title":"torch.nn.modules.module.Module.type"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemodulefloat","text":"</> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.float"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemoduledouble","text":"</> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.double"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemodulehalf","text":"</> Casts all floating point parameters and buffers to half datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.half"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemodulebfloat16","text":"</> Casts all floating point parameters and buffers to bfloat16 datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.bfloat16"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemoduleto","text":"</> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Returns (Module) self Example:: >>> linear = nn . Linear ( 2 , 2 ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]]) >>> linear . to ( torch . double ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]], dtype = torch . float64 ) >>> gpu1 = torch . device ( \"cuda:1\" ) >>> linear . to ( gpu1 , dtype = torch . half , non_blocking = True ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 , device = 'cuda:1' ) >>> cpu = torch . device ( \"cpu\" ) >>> linear . to ( cpu ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 ) method","title":"torch.nn.modules.module.Module.to"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemoduleregister_backward_hook","text":"</> Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_backward_hook"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemoduleregister_forward_pre_hook","text":"</> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_pre_hook"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemoduleregister_forward_hook","text":"</> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_hook"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemodulestate_dict","text":"</> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method","title":"torch.nn.modules.module.Module.state_dict"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemoduleload_state_dict","text":"</> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator","title":"torch.nn.modules.module.Module.load_state_dict"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemoduleparameters","text":"</> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param ), param . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.parameters"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemodulenamed_parameters","text":"</> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator","title":"torch.nn.modules.module.Module.named_parameters"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemodulebuffers","text":"</> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf ), buf . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.buffers"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemodulenamed_buffers","text":"</> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator","title":"torch.nn.modules.module.Module.named_buffers"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemodulechildren","text":"</> Returns an iterator over immediate children modules. Yields (Module) a child module generator","title":"torch.nn.modules.module.Module.children"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemodulenamed_children","text":"</> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator","title":"torch.nn.modules.module.Module.named_children"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemodulemodules","text":"</> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator","title":"torch.nn.modules.module.Module.modules"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemodulenamed_modules","text":"</> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method","title":"torch.nn.modules.module.Module.named_modules"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemoduletrain","text":"</> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.train"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemoduleeval","text":"</> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method","title":"torch.nn.modules.module.Module.eval"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemodulerequires_grad_","text":"</> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.requires_grad_"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemodulezero_grad","text":"</> Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters set_to_none (bool) \u2014 instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. method","title":"torch.nn.modules.module.Module.zero_grad"},{"location":"api/kindle.modules.bottleneck/#torchnnmodulesmodulemoduleextra_repr","text":"</> Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. method","title":"torch.nn.modules.module.Module.extra_repr"},{"location":"api/kindle.modules.bottleneck/#kindlemodulesbottleneckbottleneckforward","text":"</> Forward.","title":"kindle.modules.bottleneck.Bottleneck.forward"},{"location":"api/kindle.modules.concat/","text":"module kindle.modules . concat </> Concat module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Concat \u2014 Concatenation module. </> class kindle.modules.concat . Concat ( dimension=1 ) </> Bases torch.nn.modules.module.Module Concatenation module. Arguments: [dimension] Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> bfloat16 ( ) (Module) \u2014 Casts all floating point parameters and buffers to bfloat16 datatype. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) (str) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( x ) \u2014 Forward. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> register_backward_hook ( hook ) (RemovableHandle) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor , persistent ) \u2014 Adds a buffer to the module. </> register_forward_hook ( hook ) (RemovableHandle) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) (RemovableHandle) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> zero_grad ( set_to_none ) \u2014 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. </> method register_buffer ( name , tensor , persistent=True ) </> Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. persistent (bool) \u2014 whether the buffer is part of this module's :attr: state_dict . Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method register_parameter ( name , param ) </> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method add_module ( name , module ) </> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method apply ( fn ) </> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Returns (Module) self Example:: >>> @torch . no_grad () >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method cuda ( device=None ) </> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method cpu ( ) </> Moves all model parameters and buffers to the CPU. Returns (Module) self method type ( dst_type ) </> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method float ( ) </> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method double ( ) </> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method half ( ) </> Casts all floating point parameters and buffers to half datatype. Returns (Module) self method bfloat16 ( ) </> Casts all floating point parameters and buffers to bfloat16 datatype. Returns (Module) self method to ( *args , **kwargs ) </> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Returns (Module) self Example:: >>> linear = nn . Linear ( 2 , 2 ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]]) >>> linear . to ( torch . double ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]], dtype = torch . float64 ) >>> gpu1 = torch . device ( \"cuda:1\" ) >>> linear . to ( gpu1 , dtype = torch . half , non_blocking = True ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 , device = 'cuda:1' ) >>> cpu = torch . device ( \"cpu\" ) >>> linear . to ( cpu ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 ) method register_backward_hook ( hook ) </> Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method register_forward_pre_hook ( hook ) </> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method register_forward_hook ( hook ) </> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method state_dict ( destination=None , prefix='' , keep_vars=False ) </> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method load_state_dict ( state_dict , strict=True ) </> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator parameters ( recurse=True ) </> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param ), param . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator named_parameters ( prefix='' , recurse=True ) </> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator buffers ( recurse=True ) </> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf ), buf . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator named_buffers ( prefix='' , recurse=True ) </> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator children ( ) </> Returns an iterator over immediate children modules. Yields (Module) a child module generator named_children ( ) </> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator modules ( ) </> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator named_modules ( memo=None , prefix='' ) </> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method train ( mode=True ) </> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method eval ( ) </> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method requires_grad_ ( requires_grad=True ) </> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method zero_grad ( set_to_none=False ) </> Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters set_to_none (bool) \u2014 instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. method extra_repr ( ) \u2192 str </> Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. method forward ( x ) </> Forward.","title":"kindle.modules.concat"},{"location":"api/kindle.modules.concat/#kindlemodulesconcat","text":"</> Concat module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Concat \u2014 Concatenation module. </> class","title":"kindle.modules.concat"},{"location":"api/kindle.modules.concat/#kindlemodulesconcatconcat","text":"</> Bases torch.nn.modules.module.Module Concatenation module. Arguments: [dimension] Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> bfloat16 ( ) (Module) \u2014 Casts all floating point parameters and buffers to bfloat16 datatype. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) (str) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( x ) \u2014 Forward. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> register_backward_hook ( hook ) (RemovableHandle) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor , persistent ) \u2014 Adds a buffer to the module. </> register_forward_hook ( hook ) (RemovableHandle) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) (RemovableHandle) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> zero_grad ( set_to_none ) \u2014 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. </> method","title":"kindle.modules.concat.Concat"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemoduleregister_buffer","text":"</> Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. persistent (bool) \u2014 whether the buffer is part of this module's :attr: state_dict . Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method","title":"torch.nn.modules.module.Module.register_buffer"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemoduleregister_parameter","text":"</> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method","title":"torch.nn.modules.module.Module.register_parameter"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemoduleadd_module","text":"</> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method","title":"torch.nn.modules.module.Module.add_module"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemoduleapply","text":"</> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Returns (Module) self Example:: >>> @torch . no_grad () >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method","title":"torch.nn.modules.module.Module.apply"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemodulecuda","text":"</> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method","title":"torch.nn.modules.module.Module.cuda"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemodulecpu","text":"</> Moves all model parameters and buffers to the CPU. Returns (Module) self method","title":"torch.nn.modules.module.Module.cpu"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemoduletype","text":"</> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method","title":"torch.nn.modules.module.Module.type"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemodulefloat","text":"</> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.float"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemoduledouble","text":"</> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.double"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemodulehalf","text":"</> Casts all floating point parameters and buffers to half datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.half"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemodulebfloat16","text":"</> Casts all floating point parameters and buffers to bfloat16 datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.bfloat16"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemoduleto","text":"</> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Returns (Module) self Example:: >>> linear = nn . Linear ( 2 , 2 ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]]) >>> linear . to ( torch . double ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]], dtype = torch . float64 ) >>> gpu1 = torch . device ( \"cuda:1\" ) >>> linear . to ( gpu1 , dtype = torch . half , non_blocking = True ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 , device = 'cuda:1' ) >>> cpu = torch . device ( \"cpu\" ) >>> linear . to ( cpu ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 ) method","title":"torch.nn.modules.module.Module.to"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemoduleregister_backward_hook","text":"</> Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_backward_hook"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemoduleregister_forward_pre_hook","text":"</> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_pre_hook"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemoduleregister_forward_hook","text":"</> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_hook"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemodulestate_dict","text":"</> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method","title":"torch.nn.modules.module.Module.state_dict"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemoduleload_state_dict","text":"</> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator","title":"torch.nn.modules.module.Module.load_state_dict"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemoduleparameters","text":"</> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param ), param . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.parameters"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemodulenamed_parameters","text":"</> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator","title":"torch.nn.modules.module.Module.named_parameters"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemodulebuffers","text":"</> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf ), buf . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.buffers"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemodulenamed_buffers","text":"</> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator","title":"torch.nn.modules.module.Module.named_buffers"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemodulechildren","text":"</> Returns an iterator over immediate children modules. Yields (Module) a child module generator","title":"torch.nn.modules.module.Module.children"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemodulenamed_children","text":"</> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator","title":"torch.nn.modules.module.Module.named_children"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemodulemodules","text":"</> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator","title":"torch.nn.modules.module.Module.modules"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemodulenamed_modules","text":"</> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method","title":"torch.nn.modules.module.Module.named_modules"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemoduletrain","text":"</> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.train"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemoduleeval","text":"</> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method","title":"torch.nn.modules.module.Module.eval"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemodulerequires_grad_","text":"</> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.requires_grad_"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemodulezero_grad","text":"</> Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters set_to_none (bool) \u2014 instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. method","title":"torch.nn.modules.module.Module.zero_grad"},{"location":"api/kindle.modules.concat/#torchnnmodulesmodulemoduleextra_repr","text":"</> Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. method","title":"torch.nn.modules.module.Module.extra_repr"},{"location":"api/kindle.modules.concat/#kindlemodulesconcatconcatforward","text":"</> Forward.","title":"kindle.modules.concat.Concat.forward"},{"location":"api/kindle.modules.conv/","text":"module kindle.modules . conv </> Conv module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Conv \u2014 Standard convolution with batch normalization and activation. </> class kindle.modules.conv . Conv ( in_channels , out_channels , kernel_size , stride=1 , padding=None , groups=1 , activation='ReLU' ) </> Bases torch.nn.modules.module.Module Standard convolution with batch normalization and activation. Arguments: [channel, kernel_size, stride, padding, groups, activation] Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> bfloat16 ( ) (Module) \u2014 Casts all floating point parameters and buffers to bfloat16 datatype. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) (str) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( x ) (Tensor) \u2014 Forward. </> fusefoward ( x ) (Tensor) \u2014 Fuse forward. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> register_backward_hook ( hook ) (RemovableHandle) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor , persistent ) \u2014 Adds a buffer to the module. </> register_forward_hook ( hook ) (RemovableHandle) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) (RemovableHandle) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> zero_grad ( set_to_none ) \u2014 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. </> method register_buffer ( name , tensor , persistent=True ) </> Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. persistent (bool) \u2014 whether the buffer is part of this module's :attr: state_dict . Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method register_parameter ( name , param ) </> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method add_module ( name , module ) </> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method apply ( fn ) </> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Returns (Module) self Example:: >>> @torch . no_grad () >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method cuda ( device=None ) </> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method cpu ( ) </> Moves all model parameters and buffers to the CPU. Returns (Module) self method type ( dst_type ) </> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method float ( ) </> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method double ( ) </> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method half ( ) </> Casts all floating point parameters and buffers to half datatype. Returns (Module) self method bfloat16 ( ) </> Casts all floating point parameters and buffers to bfloat16 datatype. Returns (Module) self method to ( *args , **kwargs ) </> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Returns (Module) self Example:: >>> linear = nn . Linear ( 2 , 2 ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]]) >>> linear . to ( torch . double ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]], dtype = torch . float64 ) >>> gpu1 = torch . device ( \"cuda:1\" ) >>> linear . to ( gpu1 , dtype = torch . half , non_blocking = True ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 , device = 'cuda:1' ) >>> cpu = torch . device ( \"cpu\" ) >>> linear . to ( cpu ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 ) method register_backward_hook ( hook ) </> Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method register_forward_pre_hook ( hook ) </> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method register_forward_hook ( hook ) </> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method state_dict ( destination=None , prefix='' , keep_vars=False ) </> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method load_state_dict ( state_dict , strict=True ) </> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator parameters ( recurse=True ) </> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param ), param . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator named_parameters ( prefix='' , recurse=True ) </> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator buffers ( recurse=True ) </> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf ), buf . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator named_buffers ( prefix='' , recurse=True ) </> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator children ( ) </> Returns an iterator over immediate children modules. Yields (Module) a child module generator named_children ( ) </> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator modules ( ) </> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator named_modules ( memo=None , prefix='' ) </> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method train ( mode=True ) </> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method eval ( ) </> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method requires_grad_ ( requires_grad=True ) </> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method zero_grad ( set_to_none=False ) </> Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters set_to_none (bool) \u2014 instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. method extra_repr ( ) \u2192 str </> Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. method forward ( x ) \u2192 Tensor </> Forward. method fusefoward ( x ) \u2192 Tensor </> Fuse forward.","title":"kindle.modules.conv"},{"location":"api/kindle.modules.conv/#kindlemodulesconv","text":"</> Conv module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Conv \u2014 Standard convolution with batch normalization and activation. </> class","title":"kindle.modules.conv"},{"location":"api/kindle.modules.conv/#kindlemodulesconvconv","text":"</> Bases torch.nn.modules.module.Module Standard convolution with batch normalization and activation. Arguments: [channel, kernel_size, stride, padding, groups, activation] Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> bfloat16 ( ) (Module) \u2014 Casts all floating point parameters and buffers to bfloat16 datatype. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) (str) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( x ) (Tensor) \u2014 Forward. </> fusefoward ( x ) (Tensor) \u2014 Fuse forward. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> register_backward_hook ( hook ) (RemovableHandle) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor , persistent ) \u2014 Adds a buffer to the module. </> register_forward_hook ( hook ) (RemovableHandle) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) (RemovableHandle) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> zero_grad ( set_to_none ) \u2014 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. </> method","title":"kindle.modules.conv.Conv"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemoduleregister_buffer","text":"</> Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. persistent (bool) \u2014 whether the buffer is part of this module's :attr: state_dict . Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method","title":"torch.nn.modules.module.Module.register_buffer"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemoduleregister_parameter","text":"</> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method","title":"torch.nn.modules.module.Module.register_parameter"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemoduleadd_module","text":"</> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method","title":"torch.nn.modules.module.Module.add_module"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemoduleapply","text":"</> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Returns (Module) self Example:: >>> @torch . no_grad () >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method","title":"torch.nn.modules.module.Module.apply"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemodulecuda","text":"</> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method","title":"torch.nn.modules.module.Module.cuda"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemodulecpu","text":"</> Moves all model parameters and buffers to the CPU. Returns (Module) self method","title":"torch.nn.modules.module.Module.cpu"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemoduletype","text":"</> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method","title":"torch.nn.modules.module.Module.type"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemodulefloat","text":"</> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.float"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemoduledouble","text":"</> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.double"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemodulehalf","text":"</> Casts all floating point parameters and buffers to half datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.half"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemodulebfloat16","text":"</> Casts all floating point parameters and buffers to bfloat16 datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.bfloat16"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemoduleto","text":"</> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Returns (Module) self Example:: >>> linear = nn . Linear ( 2 , 2 ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]]) >>> linear . to ( torch . double ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]], dtype = torch . float64 ) >>> gpu1 = torch . device ( \"cuda:1\" ) >>> linear . to ( gpu1 , dtype = torch . half , non_blocking = True ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 , device = 'cuda:1' ) >>> cpu = torch . device ( \"cpu\" ) >>> linear . to ( cpu ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 ) method","title":"torch.nn.modules.module.Module.to"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemoduleregister_backward_hook","text":"</> Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_backward_hook"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemoduleregister_forward_pre_hook","text":"</> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_pre_hook"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemoduleregister_forward_hook","text":"</> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_hook"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemodulestate_dict","text":"</> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method","title":"torch.nn.modules.module.Module.state_dict"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemoduleload_state_dict","text":"</> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator","title":"torch.nn.modules.module.Module.load_state_dict"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemoduleparameters","text":"</> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param ), param . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.parameters"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemodulenamed_parameters","text":"</> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator","title":"torch.nn.modules.module.Module.named_parameters"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemodulebuffers","text":"</> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf ), buf . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.buffers"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemodulenamed_buffers","text":"</> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator","title":"torch.nn.modules.module.Module.named_buffers"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemodulechildren","text":"</> Returns an iterator over immediate children modules. Yields (Module) a child module generator","title":"torch.nn.modules.module.Module.children"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemodulenamed_children","text":"</> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator","title":"torch.nn.modules.module.Module.named_children"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemodulemodules","text":"</> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator","title":"torch.nn.modules.module.Module.modules"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemodulenamed_modules","text":"</> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method","title":"torch.nn.modules.module.Module.named_modules"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemoduletrain","text":"</> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.train"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemoduleeval","text":"</> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method","title":"torch.nn.modules.module.Module.eval"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemodulerequires_grad_","text":"</> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.requires_grad_"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemodulezero_grad","text":"</> Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters set_to_none (bool) \u2014 instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. method","title":"torch.nn.modules.module.Module.zero_grad"},{"location":"api/kindle.modules.conv/#torchnnmodulesmodulemoduleextra_repr","text":"</> Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. method","title":"torch.nn.modules.module.Module.extra_repr"},{"location":"api/kindle.modules.conv/#kindlemodulesconvconvforward","text":"</> Forward. method","title":"kindle.modules.conv.Conv.forward"},{"location":"api/kindle.modules.conv/#kindlemodulesconvconvfusefoward","text":"</> Fuse forward.","title":"kindle.modules.conv.Conv.fusefoward"},{"location":"api/kindle.modules.dwconv/","text":"module kindle.modules . dwconv </> DWConv module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes DWConv \u2014 Depthwise convolution with batch normalization and activation. </> class kindle.modules.dwconv . DWConv ( in_channels , out_channels , kernel_size , stride=1 , padding=None , activation='ReLU' ) </> Bases torch.nn.modules.module.Module Depthwise convolution with batch normalization and activation. Arguments: [channel, kernel_size, stride, padding, activation] Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> bfloat16 ( ) (Module) \u2014 Casts all floating point parameters and buffers to bfloat16 datatype. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) (str) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( x ) (Tensor) \u2014 Forward. </> fusefoward ( x ) (Tensor) \u2014 Fuse forward. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> register_backward_hook ( hook ) (RemovableHandle) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor , persistent ) \u2014 Adds a buffer to the module. </> register_forward_hook ( hook ) (RemovableHandle) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) (RemovableHandle) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> zero_grad ( set_to_none ) \u2014 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. </> method register_buffer ( name , tensor , persistent=True ) </> Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. persistent (bool) \u2014 whether the buffer is part of this module's :attr: state_dict . Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method register_parameter ( name , param ) </> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method add_module ( name , module ) </> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method apply ( fn ) </> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Returns (Module) self Example:: >>> @torch . no_grad () >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method cuda ( device=None ) </> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method cpu ( ) </> Moves all model parameters and buffers to the CPU. Returns (Module) self method type ( dst_type ) </> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method float ( ) </> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method double ( ) </> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method half ( ) </> Casts all floating point parameters and buffers to half datatype. Returns (Module) self method bfloat16 ( ) </> Casts all floating point parameters and buffers to bfloat16 datatype. Returns (Module) self method to ( *args , **kwargs ) </> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Returns (Module) self Example:: >>> linear = nn . Linear ( 2 , 2 ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]]) >>> linear . to ( torch . double ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]], dtype = torch . float64 ) >>> gpu1 = torch . device ( \"cuda:1\" ) >>> linear . to ( gpu1 , dtype = torch . half , non_blocking = True ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 , device = 'cuda:1' ) >>> cpu = torch . device ( \"cpu\" ) >>> linear . to ( cpu ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 ) method register_backward_hook ( hook ) </> Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method register_forward_pre_hook ( hook ) </> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method register_forward_hook ( hook ) </> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method state_dict ( destination=None , prefix='' , keep_vars=False ) </> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method load_state_dict ( state_dict , strict=True ) </> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator parameters ( recurse=True ) </> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param ), param . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator named_parameters ( prefix='' , recurse=True ) </> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator buffers ( recurse=True ) </> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf ), buf . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator named_buffers ( prefix='' , recurse=True ) </> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator children ( ) </> Returns an iterator over immediate children modules. Yields (Module) a child module generator named_children ( ) </> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator modules ( ) </> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator named_modules ( memo=None , prefix='' ) </> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method train ( mode=True ) </> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method eval ( ) </> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method requires_grad_ ( requires_grad=True ) </> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method zero_grad ( set_to_none=False ) </> Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters set_to_none (bool) \u2014 instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. method extra_repr ( ) \u2192 str </> Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. method forward ( x ) \u2192 Tensor </> Forward. method fusefoward ( x ) \u2192 Tensor </> Fuse forward.","title":"kindle.modules.dwconv"},{"location":"api/kindle.modules.dwconv/#kindlemodulesdwconv","text":"</> DWConv module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes DWConv \u2014 Depthwise convolution with batch normalization and activation. </> class","title":"kindle.modules.dwconv"},{"location":"api/kindle.modules.dwconv/#kindlemodulesdwconvdwconv","text":"</> Bases torch.nn.modules.module.Module Depthwise convolution with batch normalization and activation. Arguments: [channel, kernel_size, stride, padding, activation] Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> bfloat16 ( ) (Module) \u2014 Casts all floating point parameters and buffers to bfloat16 datatype. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) (str) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( x ) (Tensor) \u2014 Forward. </> fusefoward ( x ) (Tensor) \u2014 Fuse forward. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> register_backward_hook ( hook ) (RemovableHandle) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor , persistent ) \u2014 Adds a buffer to the module. </> register_forward_hook ( hook ) (RemovableHandle) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) (RemovableHandle) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> zero_grad ( set_to_none ) \u2014 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. </> method","title":"kindle.modules.dwconv.DWConv"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemoduleregister_buffer","text":"</> Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. persistent (bool) \u2014 whether the buffer is part of this module's :attr: state_dict . Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method","title":"torch.nn.modules.module.Module.register_buffer"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemoduleregister_parameter","text":"</> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method","title":"torch.nn.modules.module.Module.register_parameter"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemoduleadd_module","text":"</> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method","title":"torch.nn.modules.module.Module.add_module"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemoduleapply","text":"</> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Returns (Module) self Example:: >>> @torch . no_grad () >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method","title":"torch.nn.modules.module.Module.apply"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemodulecuda","text":"</> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method","title":"torch.nn.modules.module.Module.cuda"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemodulecpu","text":"</> Moves all model parameters and buffers to the CPU. Returns (Module) self method","title":"torch.nn.modules.module.Module.cpu"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemoduletype","text":"</> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method","title":"torch.nn.modules.module.Module.type"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemodulefloat","text":"</> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.float"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemoduledouble","text":"</> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.double"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemodulehalf","text":"</> Casts all floating point parameters and buffers to half datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.half"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemodulebfloat16","text":"</> Casts all floating point parameters and buffers to bfloat16 datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.bfloat16"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemoduleto","text":"</> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Returns (Module) self Example:: >>> linear = nn . Linear ( 2 , 2 ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]]) >>> linear . to ( torch . double ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]], dtype = torch . float64 ) >>> gpu1 = torch . device ( \"cuda:1\" ) >>> linear . to ( gpu1 , dtype = torch . half , non_blocking = True ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 , device = 'cuda:1' ) >>> cpu = torch . device ( \"cpu\" ) >>> linear . to ( cpu ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 ) method","title":"torch.nn.modules.module.Module.to"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemoduleregister_backward_hook","text":"</> Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_backward_hook"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemoduleregister_forward_pre_hook","text":"</> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_pre_hook"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemoduleregister_forward_hook","text":"</> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_hook"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemodulestate_dict","text":"</> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method","title":"torch.nn.modules.module.Module.state_dict"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemoduleload_state_dict","text":"</> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator","title":"torch.nn.modules.module.Module.load_state_dict"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemoduleparameters","text":"</> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param ), param . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.parameters"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemodulenamed_parameters","text":"</> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator","title":"torch.nn.modules.module.Module.named_parameters"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemodulebuffers","text":"</> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf ), buf . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.buffers"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemodulenamed_buffers","text":"</> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator","title":"torch.nn.modules.module.Module.named_buffers"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemodulechildren","text":"</> Returns an iterator over immediate children modules. Yields (Module) a child module generator","title":"torch.nn.modules.module.Module.children"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemodulenamed_children","text":"</> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator","title":"torch.nn.modules.module.Module.named_children"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemodulemodules","text":"</> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator","title":"torch.nn.modules.module.Module.modules"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemodulenamed_modules","text":"</> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method","title":"torch.nn.modules.module.Module.named_modules"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemoduletrain","text":"</> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.train"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemoduleeval","text":"</> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method","title":"torch.nn.modules.module.Module.eval"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemodulerequires_grad_","text":"</> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.requires_grad_"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemodulezero_grad","text":"</> Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters set_to_none (bool) \u2014 instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. method","title":"torch.nn.modules.module.Module.zero_grad"},{"location":"api/kindle.modules.dwconv/#torchnnmodulesmodulemoduleextra_repr","text":"</> Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. method","title":"torch.nn.modules.module.Module.extra_repr"},{"location":"api/kindle.modules.dwconv/#kindlemodulesdwconvdwconvforward","text":"</> Forward. method","title":"kindle.modules.dwconv.DWConv.forward"},{"location":"api/kindle.modules.dwconv/#kindlemodulesdwconvdwconvfusefoward","text":"</> Fuse forward.","title":"kindle.modules.dwconv.DWConv.fusefoward"},{"location":"api/kindle.modules.linear/","text":"module kindle.modules . linear </> Linear module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Linear \u2014 Linear module. </> class kindle.modules.linear . Linear ( in_channel , out_channel , activation ) </> Bases torch.nn.modules.module.Module Linear module. Arguments: [channel, activation] Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> bfloat16 ( ) (Module) \u2014 Casts all floating point parameters and buffers to bfloat16 datatype. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) (str) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( x ) (Tensor) \u2014 Forward. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> register_backward_hook ( hook ) (RemovableHandle) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor , persistent ) \u2014 Adds a buffer to the module. </> register_forward_hook ( hook ) (RemovableHandle) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) (RemovableHandle) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> zero_grad ( set_to_none ) \u2014 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. </> method register_buffer ( name , tensor , persistent=True ) </> Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. persistent (bool) \u2014 whether the buffer is part of this module's :attr: state_dict . Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method register_parameter ( name , param ) </> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method add_module ( name , module ) </> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method apply ( fn ) </> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Returns (Module) self Example:: >>> @torch . no_grad () >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method cuda ( device=None ) </> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method cpu ( ) </> Moves all model parameters and buffers to the CPU. Returns (Module) self method type ( dst_type ) </> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method float ( ) </> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method double ( ) </> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method half ( ) </> Casts all floating point parameters and buffers to half datatype. Returns (Module) self method bfloat16 ( ) </> Casts all floating point parameters and buffers to bfloat16 datatype. Returns (Module) self method to ( *args , **kwargs ) </> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Returns (Module) self Example:: >>> linear = nn . Linear ( 2 , 2 ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]]) >>> linear . to ( torch . double ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]], dtype = torch . float64 ) >>> gpu1 = torch . device ( \"cuda:1\" ) >>> linear . to ( gpu1 , dtype = torch . half , non_blocking = True ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 , device = 'cuda:1' ) >>> cpu = torch . device ( \"cpu\" ) >>> linear . to ( cpu ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 ) method register_backward_hook ( hook ) </> Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method register_forward_pre_hook ( hook ) </> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method register_forward_hook ( hook ) </> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method state_dict ( destination=None , prefix='' , keep_vars=False ) </> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method load_state_dict ( state_dict , strict=True ) </> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator parameters ( recurse=True ) </> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param ), param . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator named_parameters ( prefix='' , recurse=True ) </> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator buffers ( recurse=True ) </> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf ), buf . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator named_buffers ( prefix='' , recurse=True ) </> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator children ( ) </> Returns an iterator over immediate children modules. Yields (Module) a child module generator named_children ( ) </> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator modules ( ) </> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator named_modules ( memo=None , prefix='' ) </> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method train ( mode=True ) </> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method eval ( ) </> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method requires_grad_ ( requires_grad=True ) </> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method zero_grad ( set_to_none=False ) </> Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters set_to_none (bool) \u2014 instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. method extra_repr ( ) \u2192 str </> Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. method forward ( x ) \u2192 Tensor </> Forward.","title":"kindle.modules.linear"},{"location":"api/kindle.modules.linear/#kindlemoduleslinear","text":"</> Linear module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Linear \u2014 Linear module. </> class","title":"kindle.modules.linear"},{"location":"api/kindle.modules.linear/#kindlemoduleslinearlinear","text":"</> Bases torch.nn.modules.module.Module Linear module. Arguments: [channel, activation] Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> bfloat16 ( ) (Module) \u2014 Casts all floating point parameters and buffers to bfloat16 datatype. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) (str) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( x ) (Tensor) \u2014 Forward. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> register_backward_hook ( hook ) (RemovableHandle) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor , persistent ) \u2014 Adds a buffer to the module. </> register_forward_hook ( hook ) (RemovableHandle) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) (RemovableHandle) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> zero_grad ( set_to_none ) \u2014 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. </> method","title":"kindle.modules.linear.Linear"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemoduleregister_buffer","text":"</> Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. persistent (bool) \u2014 whether the buffer is part of this module's :attr: state_dict . Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method","title":"torch.nn.modules.module.Module.register_buffer"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemoduleregister_parameter","text":"</> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method","title":"torch.nn.modules.module.Module.register_parameter"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemoduleadd_module","text":"</> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method","title":"torch.nn.modules.module.Module.add_module"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemoduleapply","text":"</> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Returns (Module) self Example:: >>> @torch . no_grad () >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method","title":"torch.nn.modules.module.Module.apply"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemodulecuda","text":"</> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method","title":"torch.nn.modules.module.Module.cuda"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemodulecpu","text":"</> Moves all model parameters and buffers to the CPU. Returns (Module) self method","title":"torch.nn.modules.module.Module.cpu"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemoduletype","text":"</> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method","title":"torch.nn.modules.module.Module.type"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemodulefloat","text":"</> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.float"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemoduledouble","text":"</> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.double"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemodulehalf","text":"</> Casts all floating point parameters and buffers to half datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.half"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemodulebfloat16","text":"</> Casts all floating point parameters and buffers to bfloat16 datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.bfloat16"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemoduleto","text":"</> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Returns (Module) self Example:: >>> linear = nn . Linear ( 2 , 2 ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]]) >>> linear . to ( torch . double ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]], dtype = torch . float64 ) >>> gpu1 = torch . device ( \"cuda:1\" ) >>> linear . to ( gpu1 , dtype = torch . half , non_blocking = True ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 , device = 'cuda:1' ) >>> cpu = torch . device ( \"cpu\" ) >>> linear . to ( cpu ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 ) method","title":"torch.nn.modules.module.Module.to"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemoduleregister_backward_hook","text":"</> Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_backward_hook"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemoduleregister_forward_pre_hook","text":"</> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_pre_hook"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemoduleregister_forward_hook","text":"</> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_hook"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemodulestate_dict","text":"</> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method","title":"torch.nn.modules.module.Module.state_dict"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemoduleload_state_dict","text":"</> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator","title":"torch.nn.modules.module.Module.load_state_dict"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemoduleparameters","text":"</> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param ), param . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.parameters"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemodulenamed_parameters","text":"</> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator","title":"torch.nn.modules.module.Module.named_parameters"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemodulebuffers","text":"</> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf ), buf . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.buffers"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemodulenamed_buffers","text":"</> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator","title":"torch.nn.modules.module.Module.named_buffers"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemodulechildren","text":"</> Returns an iterator over immediate children modules. Yields (Module) a child module generator","title":"torch.nn.modules.module.Module.children"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemodulenamed_children","text":"</> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator","title":"torch.nn.modules.module.Module.named_children"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemodulemodules","text":"</> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator","title":"torch.nn.modules.module.Module.modules"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemodulenamed_modules","text":"</> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method","title":"torch.nn.modules.module.Module.named_modules"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemoduletrain","text":"</> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.train"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemoduleeval","text":"</> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method","title":"torch.nn.modules.module.Module.eval"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemodulerequires_grad_","text":"</> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.requires_grad_"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemodulezero_grad","text":"</> Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters set_to_none (bool) \u2014 instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. method","title":"torch.nn.modules.module.Module.zero_grad"},{"location":"api/kindle.modules.linear/#torchnnmodulesmodulemoduleextra_repr","text":"</> Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. method","title":"torch.nn.modules.module.Module.extra_repr"},{"location":"api/kindle.modules.linear/#kindlemoduleslinearlinearforward","text":"</> Forward.","title":"kindle.modules.linear.Linear.forward"},{"location":"api/kindle.modules/","text":"package kindle . modules </> PyTorch Modules. module kindle.modules . dwconv </> DWConv module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes DWConv \u2014 Depthwise convolution with batch normalization and activation. </> module kindle.modules . add </> Module Description. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Add \u2014 Add module for Kindle. </> module kindle.modules . concat </> Concat module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Concat \u2014 Concatenation module. </> module kindle.modules . linear </> Linear module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Linear \u2014 Linear module. </> module kindle.modules . bottleneck </> Bottleneck(ResNet) module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Bottleneck \u2014 Standard bottleneck block. </> module kindle.modules . conv </> Conv module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Conv \u2014 Standard convolution with batch normalization and activation. </> module kindle.modules . poolings </> Module generator related to pooling operations. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes GlobalAvgPool \u2014 Global average pooling module. </>","title":"kindle.modules"},{"location":"api/kindle.modules/#kindlemodules","text":"</> PyTorch Modules. module","title":"kindle.modules"},{"location":"api/kindle.modules/#kindlemodulesdwconv","text":"</> DWConv module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes DWConv \u2014 Depthwise convolution with batch normalization and activation. </> module","title":"kindle.modules.dwconv"},{"location":"api/kindle.modules/#kindlemodulesadd","text":"</> Module Description. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Add \u2014 Add module for Kindle. </> module","title":"kindle.modules.add"},{"location":"api/kindle.modules/#kindlemodulesconcat","text":"</> Concat module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Concat \u2014 Concatenation module. </> module","title":"kindle.modules.concat"},{"location":"api/kindle.modules/#kindlemoduleslinear","text":"</> Linear module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Linear \u2014 Linear module. </> module","title":"kindle.modules.linear"},{"location":"api/kindle.modules/#kindlemodulesbottleneck","text":"</> Bottleneck(ResNet) module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Bottleneck \u2014 Standard bottleneck block. </> module","title":"kindle.modules.bottleneck"},{"location":"api/kindle.modules/#kindlemodulesconv","text":"</> Conv module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Conv \u2014 Standard convolution with batch normalization and activation. </> module","title":"kindle.modules.conv"},{"location":"api/kindle.modules/#kindlemodulespoolings","text":"</> Module generator related to pooling operations. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes GlobalAvgPool \u2014 Global average pooling module. </>","title":"kindle.modules.poolings"},{"location":"api/kindle.modules.poolings/","text":"module kindle.modules . poolings </> Module generator related to pooling operations. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes GlobalAvgPool \u2014 Global average pooling module. </> class kindle.modules.poolings . GlobalAvgPool ( ) </> Bases torch.nn.modules.pooling.AdaptiveAvgPool2d torch.nn.modules.pooling._AdaptiveAvgPoolNd torch.nn.modules.module.Module Global average pooling module. Arguments: [] Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> bfloat16 ( ) (Module) \u2014 Casts all floating point parameters and buffers to bfloat16 datatype. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) (str) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( input ) (Tensor) \u2014 Defines the computation performed at every call. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> register_backward_hook ( hook ) (RemovableHandle) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor , persistent ) \u2014 Adds a buffer to the module. </> register_forward_hook ( hook ) (RemovableHandle) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) (RemovableHandle) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> zero_grad ( set_to_none ) \u2014 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. </> method register_buffer ( name , tensor , persistent=True ) </> Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. persistent (bool) \u2014 whether the buffer is part of this module's :attr: state_dict . Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method register_parameter ( name , param ) </> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method add_module ( name , module ) </> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method apply ( fn ) </> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Returns (Module) self Example:: >>> @torch . no_grad () >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method cuda ( device=None ) </> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method cpu ( ) </> Moves all model parameters and buffers to the CPU. Returns (Module) self method type ( dst_type ) </> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method float ( ) </> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method double ( ) </> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method half ( ) </> Casts all floating point parameters and buffers to half datatype. Returns (Module) self method bfloat16 ( ) </> Casts all floating point parameters and buffers to bfloat16 datatype. Returns (Module) self method to ( *args , **kwargs ) </> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Returns (Module) self Example:: >>> linear = nn . Linear ( 2 , 2 ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]]) >>> linear . to ( torch . double ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]], dtype = torch . float64 ) >>> gpu1 = torch . device ( \"cuda:1\" ) >>> linear . to ( gpu1 , dtype = torch . half , non_blocking = True ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 , device = 'cuda:1' ) >>> cpu = torch . device ( \"cpu\" ) >>> linear . to ( cpu ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 ) method register_backward_hook ( hook ) </> Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method register_forward_pre_hook ( hook ) </> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method register_forward_hook ( hook ) </> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method state_dict ( destination=None , prefix='' , keep_vars=False ) </> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method load_state_dict ( state_dict , strict=True ) </> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator parameters ( recurse=True ) </> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param ), param . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator named_parameters ( prefix='' , recurse=True ) </> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator buffers ( recurse=True ) </> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf ), buf . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator named_buffers ( prefix='' , recurse=True ) </> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator children ( ) </> Returns an iterator over immediate children modules. Yields (Module) a child module generator named_children ( ) </> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator modules ( ) </> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator named_modules ( memo=None , prefix='' ) </> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method train ( mode=True ) </> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method eval ( ) </> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method requires_grad_ ( requires_grad=True ) </> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method zero_grad ( set_to_none=False ) </> Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters set_to_none (bool) \u2014 instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. method extra_repr ( ) \u2192 str </> Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. method forward ( input ) \u2192 Tensor </> Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"kindle.modules.poolings"},{"location":"api/kindle.modules.poolings/#kindlemodulespoolings","text":"</> Module generator related to pooling operations. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes GlobalAvgPool \u2014 Global average pooling module. </> class","title":"kindle.modules.poolings"},{"location":"api/kindle.modules.poolings/#kindlemodulespoolingsglobalavgpool","text":"</> Bases torch.nn.modules.pooling.AdaptiveAvgPool2d torch.nn.modules.pooling._AdaptiveAvgPoolNd torch.nn.modules.module.Module Global average pooling module. Arguments: [] Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> bfloat16 ( ) (Module) \u2014 Casts all floating point parameters and buffers to bfloat16 datatype. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) (str) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( input ) (Tensor) \u2014 Defines the computation performed at every call. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> register_backward_hook ( hook ) (RemovableHandle) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor , persistent ) \u2014 Adds a buffer to the module. </> register_forward_hook ( hook ) (RemovableHandle) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) (RemovableHandle) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> zero_grad ( set_to_none ) \u2014 Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. </> method","title":"kindle.modules.poolings.GlobalAvgPool"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemoduleregister_buffer","text":"</> Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. persistent (bool) \u2014 whether the buffer is part of this module's :attr: state_dict . Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method","title":"torch.nn.modules.module.Module.register_buffer"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemoduleregister_parameter","text":"</> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method","title":"torch.nn.modules.module.Module.register_parameter"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemoduleadd_module","text":"</> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method","title":"torch.nn.modules.module.Module.add_module"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemoduleapply","text":"</> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Returns (Module) self Example:: >>> @torch . no_grad () >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method","title":"torch.nn.modules.module.Module.apply"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemodulecuda","text":"</> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method","title":"torch.nn.modules.module.Module.cuda"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemodulecpu","text":"</> Moves all model parameters and buffers to the CPU. Returns (Module) self method","title":"torch.nn.modules.module.Module.cpu"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemoduletype","text":"</> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method","title":"torch.nn.modules.module.Module.type"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemodulefloat","text":"</> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.float"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemoduledouble","text":"</> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.double"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemodulehalf","text":"</> Casts all floating point parameters and buffers to half datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.half"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemodulebfloat16","text":"</> Casts all floating point parameters and buffers to bfloat16 datatype. Returns (Module) self method","title":"torch.nn.modules.module.Module.bfloat16"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemoduleto","text":"</> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Returns (Module) self Example:: >>> linear = nn . Linear ( 2 , 2 ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]]) >>> linear . to ( torch . double ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1913 , - 0.3420 ], [ - 0.5113 , - 0.2325 ]], dtype = torch . float64 ) >>> gpu1 = torch . device ( \"cuda:1\" ) >>> linear . to ( gpu1 , dtype = torch . half , non_blocking = True ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 , device = 'cuda:1' ) >>> cpu = torch . device ( \"cpu\" ) >>> linear . to ( cpu ) Linear ( in_features = 2 , out_features = 2 , bias = True ) >>> linear . weight Parameter containing : tensor ([[ 0.1914 , - 0.3420 ], [ - 0.5112 , - 0.2324 ]], dtype = torch . float16 ) method","title":"torch.nn.modules.module.Module.to"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemoduleregister_backward_hook","text":"</> Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_backward_hook"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemoduleregister_forward_pre_hook","text":"</> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_pre_hook"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemoduleregister_forward_hook","text":"</> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns (RemovableHandle) class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_hook"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemodulestate_dict","text":"</> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method","title":"torch.nn.modules.module.Module.state_dict"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemoduleload_state_dict","text":"</> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator","title":"torch.nn.modules.module.Module.load_state_dict"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemoduleparameters","text":"</> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param ), param . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.parameters"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemodulenamed_parameters","text":"</> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator","title":"torch.nn.modules.module.Module.named_parameters"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemodulebuffers","text":"</> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf ), buf . size ()) < class ' torch . Tensor '> (20L,) < class ' torch . Tensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.buffers"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemodulenamed_buffers","text":"</> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator","title":"torch.nn.modules.module.Module.named_buffers"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemodulechildren","text":"</> Returns an iterator over immediate children modules. Yields (Module) a child module generator","title":"torch.nn.modules.module.Module.children"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemodulenamed_children","text":"</> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator","title":"torch.nn.modules.module.Module.named_children"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemodulemodules","text":"</> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator","title":"torch.nn.modules.module.Module.modules"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemodulenamed_modules","text":"</> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method","title":"torch.nn.modules.module.Module.named_modules"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemoduletrain","text":"</> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.train"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemoduleeval","text":"</> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method","title":"torch.nn.modules.module.Module.eval"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemodulerequires_grad_","text":"</> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.requires_grad_"},{"location":"api/kindle.modules.poolings/#torchnnmodulesmodulemodulezero_grad","text":"</> Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters set_to_none (bool) \u2014 instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. method","title":"torch.nn.modules.module.Module.zero_grad"},{"location":"api/kindle.modules.poolings/#torchnnmodulespooling_adaptiveavgpoolndextra_repr","text":"</> Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. method","title":"torch.nn.modules.pooling._AdaptiveAvgPoolNd.extra_repr"},{"location":"api/kindle.modules.poolings/#torchnnmodulespoolingadaptiveavgpool2dforward","text":"</> Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"torch.nn.modules.pooling.AdaptiveAvgPool2d.forward"},{"location":"api/kindle.torch_utils/","text":"module kindle . torch_utils </> Common utility functions. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Activation \u2014 Convert string activation name to the activation class. </> Functions autopad ( kernel_size , padding ) (Union(int, list of int)) \u2014 Auto padding calculation for pad='same' in TensorFlow. </> count_model_params ( model ) (int) \u2014 Count model's parameters. </> make_divisible ( n_channel , divisor ) (int) \u2014 Convert {n_channel} to divisible by {divisor} </> model_info ( model , verbose ) \u2014 Print out model info. </> split_dataset_index ( n_data , split_ratio ) (SubsetRandomSampler, SubsetRandomSampler) \u2014 Split dataset indices with split_ratio. </> function kindle.torch_utils . split_dataset_index ( n_data , split_ratio=0.1 ) </> Split dataset indices with split_ratio. Parameters n_data (int) \u2014 number of total data split_ratio (float, optional) \u2014 split ratio (0.0 ~ 1.0) Returns (SubsetRandomSampler, SubsetRandomSampler) SubsetRandomSampler ({split_ratio} ~ 1.0) SubsetRandomSampler (0 ~ {split_ratio}) function kindle.torch_utils . model_info ( model , verbose=False ) </> Print out model info. function kindle.torch_utils . make_divisible ( n_channel , divisor=8 ) </> Convert {n_channel} to divisible by {divisor} Parameters n_channel (int or float) \u2014 number of channels. divisor (int, optional) \u2014 divisor to be used. Returns (int) Ex) n_channel=22, divisor=8 ceil(22/8) * 8 = 24 function kindle.torch_utils . autopad ( kernel_size , padding=None ) \u2192 Union(int, list of int) </> Auto padding calculation for pad='same' in TensorFlow. class kindle.torch_utils . Activation ( act_type ) </> Convert string activation name to the activation class. function kindle.torch_utils . count_model_params ( model ) \u2192 int </> Count model's parameters.","title":"kindle.torch_utils"},{"location":"api/kindle.torch_utils/#kindletorch_utils","text":"</> Common utility functions. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes Activation \u2014 Convert string activation name to the activation class. </> Functions autopad ( kernel_size , padding ) (Union(int, list of int)) \u2014 Auto padding calculation for pad='same' in TensorFlow. </> count_model_params ( model ) (int) \u2014 Count model's parameters. </> make_divisible ( n_channel , divisor ) (int) \u2014 Convert {n_channel} to divisible by {divisor} </> model_info ( model , verbose ) \u2014 Print out model info. </> split_dataset_index ( n_data , split_ratio ) (SubsetRandomSampler, SubsetRandomSampler) \u2014 Split dataset indices with split_ratio. </> function","title":"kindle.torch_utils"},{"location":"api/kindle.torch_utils/#kindletorch_utilssplit_dataset_index","text":"</> Split dataset indices with split_ratio. Parameters n_data (int) \u2014 number of total data split_ratio (float, optional) \u2014 split ratio (0.0 ~ 1.0) Returns (SubsetRandomSampler, SubsetRandomSampler) SubsetRandomSampler ({split_ratio} ~ 1.0) SubsetRandomSampler (0 ~ {split_ratio}) function","title":"kindle.torch_utils.split_dataset_index"},{"location":"api/kindle.torch_utils/#kindletorch_utilsmodel_info","text":"</> Print out model info. function","title":"kindle.torch_utils.model_info"},{"location":"api/kindle.torch_utils/#kindletorch_utilsmake_divisible","text":"</> Convert {n_channel} to divisible by {divisor} Parameters n_channel (int or float) \u2014 number of channels. divisor (int, optional) \u2014 divisor to be used. Returns (int) Ex) n_channel=22, divisor=8 ceil(22/8) * 8 = 24 function","title":"kindle.torch_utils.make_divisible"},{"location":"api/kindle.torch_utils/#kindletorch_utilsautopad","text":"</> Auto padding calculation for pad='same' in TensorFlow. class","title":"kindle.torch_utils.autopad"},{"location":"api/kindle.torch_utils/#kindletorch_utilsactivation","text":"</> Convert string activation name to the activation class. function","title":"kindle.torch_utils.Activation"},{"location":"api/kindle.torch_utils/#kindletorch_utilscount_model_params","text":"</> Count model's parameters.","title":"kindle.torch_utils.count_model_params"},{"location":"api/kindle.trainer/","text":"module kindle . trainer </> PyTorch trainer module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes TorchTrainer \u2014 Pytorch Trainer. </> class kindle.trainer . TorchTrainer ( model , criterion , optimizer , device='cpu' , verbose=1 ) </> Pytorch Trainer. Methods test ( test_dataloader ) (float, float) \u2014 Test model. </> train ( train_dataloader , n_epoch , shuffle , test_dataloader ) (float, float) \u2014 Train model. </> method train ( train_dataloader , n_epoch , shuffle=False , test_dataloader=None ) </> Train model. Parameters train_dataloader (DataLoader) \u2014 data loader module which is a iterator that returns (data, labels) n_epoch (int) \u2014 number of total epochs for training shuffle (bool, optional) \u2014 shuffle train data on every epoch. Sampler must be SubsetRandomSampler to apply shuffle. test_dataloader (DataLoader, optional) \u2014 test data loader Returns (float, float) loss and accuracy method test ( test_dataloader ) </> Test model. Parameters test_dataloader (DataLoader) \u2014 test data loader module which is a iterator that returns (data, labels) Returns (float, float) loss, accuracy","title":"kindle.trainer"},{"location":"api/kindle.trainer/#kindletrainer","text":"</> PyTorch trainer module. Author: Jongkuk Lim Contact: lim.jeikei@gmail.com Classes TorchTrainer \u2014 Pytorch Trainer. </> class","title":"kindle.trainer"},{"location":"api/kindle.trainer/#kindletrainertorchtrainer","text":"</> Pytorch Trainer. Methods test ( test_dataloader ) (float, float) \u2014 Test model. </> train ( train_dataloader , n_epoch , shuffle , test_dataloader ) (float, float) \u2014 Train model. </> method","title":"kindle.trainer.TorchTrainer"},{"location":"api/kindle.trainer/#kindletrainertorchtrainertrain","text":"</> Train model. Parameters train_dataloader (DataLoader) \u2014 data loader module which is a iterator that returns (data, labels) n_epoch (int) \u2014 number of total epochs for training shuffle (bool, optional) \u2014 shuffle train data on every epoch. Sampler must be SubsetRandomSampler to apply shuffle. test_dataloader (DataLoader, optional) \u2014 test data loader Returns (float, float) loss and accuracy method","title":"kindle.trainer.TorchTrainer.train"},{"location":"api/kindle.trainer/#kindletrainertorchtrainertest","text":"</> Test model. Parameters test_dataloader (DataLoader) \u2014 test data loader module which is a iterator that returns (data, labels) Returns (float, float) loss, accuracy","title":"kindle.trainer.TorchTrainer.test"},{"location":"api/source/kindle.generator.add/","text":"SOURCE CODE kindle.generator. add DOCS \"\"\"Module Description. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" from typing import List import numpy as np from torch import nn from kindle.generator.base_generator import GeneratorAbstract from kindle.modules.add import Add class AddGenerator ( GeneratorAbstract ): DOCS \"\"\"Add module generator.\"\"\" def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Add module generator.\"\"\" super () . __init__ ( * args , ** kwargs ) @property def out_channel ( self ) -> int : if isinstance ( self . from_idx , int ): raise Exception ( \"Add must have more than 2 inputs.\" ) return self . in_channels [ self . from_idx [ 0 ]] @property def in_channel ( self ) -> int : return self . out_channel def compute_out_shape ( self , size : np . ndarray , repeat : int = 1 ) -> List [ int ]: DOCS return list ( size ) def __call__ ( self , repeat : int = 1 ) -> nn . Module : DOCS module = Add () return self . _get_module ( module )","title":"kindle.generator.add"},{"location":"api/source/kindle.generator.base_generator/","text":"SOURCE CODE kindle.generator. base_generator DOCS \"\"\"Base Module Generator. This module is responsible for GeneratorAbstract and ModuleGenerator. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" from abc import ABC , abstractmethod from typing import List , Optional , Tuple , Union import numpy as np from torch import nn from kindle.torch_utils import make_divisible class GeneratorAbstract ( ABC ): DOCS \"\"\"Abstract Module Generator.\"\"\" CHANNEL_DIVISOR : int = 8 def __init__ ( self , * args , from_idx : Union [ int , List [ int ]] = - 1 , in_channels : Tuple [ int ] = ( 0 ,), width_multiply : float = 1.0 , ): \"\"\"Initialize module generator. Args: *args: Module arguments from_idx: Module input index in_channels: Number of input channel width_multiply: Channel width multiply \"\"\" self . args = tuple ( args ) self . from_idx = from_idx self . in_channels = in_channels self . width_multiply = width_multiply @property DOCS def name ( self ) -> str : \"\"\"Module name.\"\"\" return self . __class__ . __name__ . replace ( \"Generator\" , \"\" ) def _get_module ( self , module : Union [ nn . Module , List [ nn . Module ]]) -> nn . Module : \"\"\"Get module from __call__ function.\"\"\" if isinstance ( module , list ): module = nn . Sequential ( * module ) # error: Incompatible types in assignment (expression has type \"Union[Tensor, Module, int]\", # variable has type \"Union[Tensor, Module]\") # error: List comprehension has incompatible type List[int]; # expected List[Union[Tensor, Module]] module . n_params = sum ([ x . numel () for x in module . parameters ()]) # type: ignore # error: Cannot assign to a method module . type = self . name # type: ignore return module @classmethod def _get_divisible_channel ( cls , n_channel : int ) -> int : \"\"\"Get divisible channel by default divisor. Args: n_channel: number of channel. Returns: Ex) given {n_channel} is 52 and {GeneratorAbstract.CHANNEL_DIVISOR} is 8., return channel is 56 since ceil(52/8) = 7 and 7*8 = 56 \"\"\" return make_divisible ( n_channel , divisor = cls . CHANNEL_DIVISOR ) @property DOCS @abstractmethod def out_channel ( self ) -> int : \"\"\"Out channel of the module.\"\"\" @property DOCS @abstractmethod def in_channel ( self ) -> int : \"\"\"In channel of the module.\"\"\" @abstractmethod DOCS def compute_out_shape ( self , size : np . ndarray , repeat : int = 1 ) -> List [ int ]: \"\"\"Compute output shape when {size} is given. Args: input size to compute output shape. \"\"\" @abstractmethod DOCS def __call__ ( self , repeat : int = 1 ) -> nn . Module : \"\"\"Returns nn.Module component.\"\"\" class ModuleGenerator : DOCS \"\"\"Module generator class.\"\"\" def __init__ ( self , module_name : str , custom_module_paths : Optional [ Union [ str , List ]] = None ): \"\"\"Generate module based on the {module_name} Args: module_name: {module_name}Generator class must have been implemented. custom_module_paths: paths to find custom module generators. Default location to find module generator is 'kindle.generator'. If {custom_module_paths} is provided, ModuleGenerator will expand its search area. \"\"\" self . module_name = module_name self . generator_paths = [ \"kindle.generator\" ] if custom_module_paths is not None : if isinstance ( custom_module_paths , str ): paths = [ custom_module_paths ] else : paths = custom_module_paths self . generator_paths += paths def __call__ ( self , * args , ** kwargs ): generator_name = f \" { self . module_name } Generator\" for path in self . generator_paths : if hasattr ( __import__ ( path , fromlist = [ \"\" ]), generator_name ): return getattr ( __import__ ( path , fromlist = [ \"\" ]), generator_name )( * args , ** kwargs ) raise Exception ( f \" { generator_name } can not be found.\" )","title":"kindle.generator.base_generator"},{"location":"api/source/kindle.generator.bottleneck/","text":"SOURCE CODE kindle.generator. bottleneck DOCS \"\"\"Bottleneck module generator. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" from typing import List , Union import numpy as np import torch from torch import nn from kindle.generator.base_generator import GeneratorAbstract class BottleneckGenerator ( GeneratorAbstract ): DOCS \"\"\"Bottleneck block generator.\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) @property DOCS def out_channel ( self ) -> int : \"\"\"Get out channel size.\"\"\" return self . _get_divisible_channel ( self . args [ 0 ] * self . width_multiply ) @property DOCS def in_channel ( self ) -> int : \"\"\"Get in channel size.\"\"\" # error: Value of type \"Optional[List[int]]\" is not indexable return self . in_channels [ self . from_idx ] # type: ignore @property DOCS def base_module ( self ) -> nn . Module : \"\"\"Returns module class from kindle.common_modules based on the class name.\"\"\" return getattr ( __import__ ( \"kindle.modules\" , fromlist = [ \"\" ]), self . name ) def compute_out_shape ( DOCS self , size : Union [ list , np . ndarray ], repeat : int = 1 ) -> List [ int ]: \"\"\"Compute output shape.\"\"\" with torch . no_grad (): module : nn . Module = self ( repeat = repeat ) module_out : torch . Tensor = module ( torch . zeros ([ 1 , * list ( size )])) return list ( module_out . shape [ - 3 :]) def __call__ ( self , repeat : int = 1 ): DOCS args = [ self . in_channel , self . out_channel , * self . args [ 1 :]] module = self . base_module ( * args ) return self . _get_module ( module )","title":"kindle.generator.bottleneck"},{"location":"api/source/kindle.generator.concat/","text":"SOURCE CODE kindle.generator. concat DOCS \"\"\"Concat module generator. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" from typing import List import numpy as np from kindle.generator.base_generator import GeneratorAbstract from kindle.modules import Concat class ConcatGenerator ( GeneratorAbstract ): DOCS \"\"\"Concatenation module generator.\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) @property DOCS def out_channel ( self ) -> int : \"\"\"Get out channel size.\"\"\" # error: Value of type \"Optional[List[int]]\" is not indexable # error: Item \"int\" of \"Union[int, List[int]]\" has no attribute # \"__iter__\" (not iterable) return sum ([ self . in_channels [ i ] for i in self . from_idx ]) # type: ignore @property DOCS def in_channel ( self ) -> int : \"\"\"Get in channel size.\"\"\" return - 1 def compute_out_shape ( self , size : np . ndarray , repeat : int = 1 ) -> List [ int ]: DOCS \"\"\"Compute out shape.\"\"\" return [ self . out_channel ] + list ( size [ 0 ][ 1 :]) def __call__ ( self , repeat : int = 1 ): DOCS module = Concat ( * self . args ) return self . _get_module ( module )","title":"kindle.generator.concat"},{"location":"api/source/kindle.generator.conv/","text":"SOURCE CODE kindle.generator. conv DOCS \"\"\"Conv module generator. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" from typing import List , Union import numpy as np import torch from torch import nn from kindle.generator.base_generator import GeneratorAbstract class ConvGenerator ( GeneratorAbstract ): DOCS \"\"\"Conv2d generator for parsing module.\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) @property DOCS def out_channel ( self ) -> int : \"\"\"Get out channel size.\"\"\" return self . _get_divisible_channel ( self . args [ 0 ] * self . width_multiply ) @property DOCS def in_channel ( self ) -> int : \"\"\"Get in channel size.\"\"\" # error: Value of type \"Optional[List[int]]\" is not indexable return self . in_channels [ self . from_idx ] # type: ignore @property DOCS def base_module ( self ) -> nn . Module : \"\"\"Returns module class from kindle.common_modules based on the class name.\"\"\" return getattr ( __import__ ( \"kindle.modules\" , fromlist = [ \"\" ]), self . name ) @torch . no_grad () def compute_out_shape ( self , size : Union [ list , np . ndarray ], repeat : int = 1 ) -> List [ int ]: module : nn . Module = self ( repeat = repeat ) module . eval () module_out : torch . Tensor = module ( torch . zeros ([ 1 , * list ( size )])) return list ( module_out . shape [ - 3 :]) def __call__ ( self , repeat : int = 1 ): DOCS args = [ self . in_channel , self . out_channel , * self . args [ 1 :]] if repeat > 1 : stride = 1 # Important!: stride only applies at the end of the repeat. if len ( args ) > 2 : stride = args [ 3 ] args [ 3 ] = 1 module = [] for i in range ( repeat ): if len ( args ) > 1 and stride > 1 and i == repeat - 1 : args [ 3 ] = stride module . append ( self . base_module ( * args )) args [ 0 ] = self . out_channel else : module = self . base_module ( * args ) return self . _get_module ( module )","title":"kindle.generator.conv"},{"location":"api/source/kindle.generator.custom_yaml_module/","text":"SOURCE CODE kindle.generator. custom_yaml_module DOCS \"\"\"Module Description. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" from copy import deepcopy from typing import List , Tuple , Union import numpy as np import torch import yaml from torch import nn from kindle.generator.base_generator import GeneratorAbstract from kindle.model import Model def convert_yaml_args ( DOCS data : List , args : Union [ List , Tuple ], inplace : bool = True ) -> List : \"\"\"Convert yaml data with argument value. Args: data: list or tuple that might contain string of 'arg0', 'arg1' ... args: argument values to replace with 'arg0', 'arg1' ... inplace: if False, it will not overwrite value. Returns: overwritten values by replacing 'arg0', 'arg1' ... to args[0], args[1] ... \"\"\" if not inplace : data = deepcopy ( data ) for i , _ in enumerate ( data ): if isinstance ( data [ i ], list ): data [ i ] = convert_yaml_args ( data [ i ], args ) elif isinstance ( data [ i ], str ) and data [ i ] . startswith ( \"arg\" ): idx = int ( data [ i ][ 3 :]) data [ i ] = args [ idx ] return data class YamlModuleGenerator ( GeneratorAbstract ): DOCS \"\"\"Custom yaml module generator.\"\"\" def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Initialize YamlModuleGenerator.\"\"\" super () . __init__ ( * args , ** kwargs ) with open ( self . args [ 0 ], \"r\" ) as f : self . cfg = yaml . load ( f , yaml . SafeLoader ) for i in range ( 1 , len ( self . args )): self . cfg [ \"args\" ][ i - 1 ] = self . args [ i ] if \"args\" in self . cfg and len ( self . args ) > 1 : convert_yaml_args ( self . cfg [ \"module\" ], self . cfg [ \"args\" ]) self . cfg . update ( { \"input_channel\" : self . in_channel , \"depth_multiple\" : 1.0 , \"width_multiple\" : self . width_multiply , \"backbone\" : self . cfg . pop ( \"module\" ), } ) self . module = Model ( self . cfg , verbose = False ) @property def out_channel ( self ) -> int : temp_in_shape = np . array ([ self . cfg [ \"input_channel\" ], 128 , 128 ]) out_shape = self . compute_out_shape ( temp_in_shape ) return out_shape [ 0 ] @property def in_channel ( self ) -> int : if isinstance ( self . from_idx , int ): return self . in_channels [ self . from_idx ] return sum ([ self . in_channels [ idx ] for idx in self . from_idx ]) @torch . no_grad () def compute_out_shape ( self , size : np . ndarray , repeat : int = 1 ) -> List [ int ]: module = self ( repeat = repeat ) module . eval () module_out = module ( torch . zeros ([ 1 , * list ( size )])) return list ( module_out . shape [ - 3 :]) def __call__ ( self , repeat : int = 1 ) -> nn . Module : DOCS module : Union [ List [ nn . Module ], nn . Module ] if repeat > 1 : # Currently, yaml module must have same in and out channel in order to apply repeat. module = [ Model ( self . cfg , verbose = True ) for _ in range ( repeat )] else : module = self . module return self . _get_module ( module )","title":"kindle.generator.custom_yaml_module"},{"location":"api/source/kindle.generator.dwconv/","text":"SOURCE CODE kindle.generator. dwconv DOCS \"\"\"DWConv module generator. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" from typing import List , Union import numpy as np import torch from torch import nn from kindle.generator.base_generator import GeneratorAbstract class DWConvGenerator ( GeneratorAbstract ): DOCS \"\"\"Depth-wise convolution generator for parsing module.\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) @property DOCS def out_channel ( self ) -> int : \"\"\"Get out channel size.\"\"\" return self . _get_divisible_channel ( self . args [ 0 ] * self . width_multiply ) @property DOCS def in_channel ( self ) -> int : \"\"\"Get in channel size.\"\"\" # error: Value of type \"Optional[List[int]]\" is not indexable return self . in_channels [ self . from_idx ] # type: ignore @property DOCS def base_module ( self ) -> nn . Module : \"\"\"Returns module class from kindle.common_modules based on the class name.\"\"\" return getattr ( __import__ ( \"kindle.modules\" , fromlist = [ \"\" ]), self . name ) @torch . no_grad () def compute_out_shape ( self , size : Union [ list , np . ndarray ], repeat : int = 1 ) -> List [ int ]: module : nn . Module = self ( repeat = repeat ) module . eval () module_out : torch . Tensor = module ( torch . zeros ([ 1 , * list ( size )])) return list ( module_out . shape [ - 3 :]) def __call__ ( self , repeat : int = 1 ): DOCS args = [ self . in_channel , self . out_channel , * self . args [ 1 :]] if repeat > 1 : stride = 1 # Important!: stride only applies at the end of the repeat. if len ( args ) > 2 : stride = args [ 3 ] args [ 3 ] = 1 module = [] for i in range ( repeat ): if len ( args ) > 1 and stride > 1 and i == repeat - 1 : args [ 3 ] = stride module . append ( self . base_module ( * args )) args [ 0 ] = self . out_channel else : module = self . base_module ( * args ) return self . _get_module ( module )","title":"kindle.generator.dwconv"},{"location":"api/source/kindle.generator.flatten/","text":"SOURCE CODE kindle.generator. flatten DOCS \"\"\"Flatten module generator. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" import math from typing import List import numpy as np from torch import nn from kindle.generator.base_generator import GeneratorAbstract class FlattenGenerator ( GeneratorAbstract ): DOCS \"\"\"Flatten module generator.\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) @property def out_channel ( self ) -> int : return math . prod ( self . args ) @property def in_channel ( self ) -> int : return - 1 def compute_out_shape ( self , size : np . ndarray , repeat : int = 1 ) -> List [ int ]: DOCS return [ self . out_channel ] def __call__ ( self , repeat : int = 1 ): DOCS return self . _get_module ( nn . Flatten ())","title":"kindle.generator.flatten"},{"location":"api/source/kindle.generator.linear/","text":"SOURCE CODE kindle.generator. linear DOCS \"\"\"Linear module generator. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" from typing import List import numpy as np from kindle.generator.base_generator import GeneratorAbstract from kindle.modules import Linear class LinearGenerator ( GeneratorAbstract ): DOCS \"\"\"Linear (fully connected) module generator for parsing.\"\"\" def __init__ ( self , * args , ** kwargs ): \"\"\"Initailize.\"\"\" super () . __init__ ( * args , ** kwargs ) @property DOCS def out_channel ( self ) -> int : \"\"\"Get out channel size.\"\"\" return self . args [ 0 ] @property DOCS def in_channel ( self ) -> int : \"\"\"Get in channel size.\"\"\" # error: Value of type \"Optional[List[int]]\" is not indexable return self . in_channels [ self . from_idx ] # type: ignore def compute_out_shape ( self , size : np . ndarray , repeat : int = 1 ) -> List [ int ]: DOCS \"\"\"Compute output shape.\"\"\" return [ self . out_channel ] def __call__ ( self , repeat : int = 1 ): DOCS # TODO: Apply repeat act = self . args [ 1 ] if len ( self . args ) > 1 else None return self . _get_module ( Linear ( self . in_channel , self . out_channel , activation = act ) )","title":"kindle.generator.linear"},{"location":"api/source/kindle.generator/","text":"SOURCE CODE kindle. generator DOCS \"\"\"PyTorch Module Generator for parsing model yaml file. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" from kindle.generator.add import AddGenerator from kindle.generator.base_generator import GeneratorAbstract , ModuleGenerator from kindle.generator.bottleneck import BottleneckGenerator from kindle.generator.concat import ConcatGenerator from kindle.generator.conv import ConvGenerator from kindle.generator.custom_yaml_module import YamlModuleGenerator from kindle.generator.dwconv import DWConvGenerator from kindle.generator.flatten import FlattenGenerator from kindle.generator.linear import LinearGenerator from kindle.generator.poolings import ( AvgPoolGenerator , GlobalAvgPoolGenerator , MaxPoolGenerator ) from kindle.generator.upsample import UpSampleGenerator __all__ = [ \"ModuleGenerator\" , \"GeneratorAbstract\" , \"BottleneckGenerator\" , \"ConcatGenerator\" , \"ConvGenerator\" , \"DWConvGenerator\" , \"FlattenGenerator\" , \"LinearGenerator\" , \"AvgPoolGenerator\" , \"GlobalAvgPoolGenerator\" , \"MaxPoolGenerator\" , \"YamlModuleGenerator\" , \"AddGenerator\" , \"UpSampleGenerator\" , ]","title":"kindle.generator"},{"location":"api/source/kindle.generator.poolings/","text":"SOURCE CODE kindle.generator. poolings DOCS \"\"\"MaxPool, AvgPool, and GlobalAvgPool modules generator. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" from typing import List import numpy as np import torch from torch import nn from kindle.generator.base_generator import GeneratorAbstract from kindle.modules import GlobalAvgPool class MaxPoolGenerator ( GeneratorAbstract ): DOCS \"\"\"Max pooling module generator.\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) @property DOCS def out_channel ( self ) -> int : \"\"\"Get out channel size.\"\"\" return self . in_channels [ self . from_idx ] # type: ignore @property DOCS def in_channel ( self ) -> int : \"\"\"Get in channel size.\"\"\" return self . in_channels [ self . from_idx ] # type: ignore @property DOCS def base_module ( self ) -> nn . Module : \"\"\"Base module.\"\"\" return getattr ( nn , f \" { self . name } 2d\" ) def compute_out_shape ( self , size : np . ndarray , repeat : int = 1 ) -> List [ int ]: DOCS \"\"\"Compute out shape.\"\"\" with torch . no_grad (): module : nn . Module = self ( repeat = repeat ) module_out : torch . Tensor = module ( torch . zeros ([ 1 , * list ( size )])) return list ( module_out . shape [ - 3 :]) def __call__ ( self , repeat : int = 1 ): DOCS module = ( [ self . base_module ( * self . args ) for _ in range ( repeat )] if repeat > 1 else self . base_module ( * self . args ) ) return self . _get_module ( module ) class AvgPoolGenerator ( MaxPoolGenerator ): DOCS \"\"\"Average pooling module generator.\"\"\" class GlobalAvgPoolGenerator ( GeneratorAbstract ): DOCS \"\"\"Global average pooling module generator.\"\"\" @property DOCS def out_channel ( self ) -> int : \"\"\"Get out channel size.\"\"\" if isinstance ( self . from_idx , int ): return self . in_channels [ self . from_idx ] raise Exception () @property DOCS def in_channel ( self ) -> int : \"\"\"Get in channel size.\"\"\" if isinstance ( self . from_idx , int ): return self . in_channels [ self . from_idx ] raise Exception () def compute_out_shape ( self , size : np . ndarray , repeat : int = 1 ) -> List [ int ]: DOCS \"\"\"Compute out shape.\"\"\" return [ self . out_channel , 1 , 1 ] def __call__ ( self , repeat : int = 1 ): DOCS return self . _get_module ( GlobalAvgPool ())","title":"kindle.generator.poolings"},{"location":"api/source/kindle.generator.upsample/","text":"SOURCE CODE kindle.generator. upsample DOCS \"\"\"UpSample module generator. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" from typing import List import numpy as np import torch import torch.nn as nn from kindle.generator.base_generator import GeneratorAbstract class UpSampleGenerator ( GeneratorAbstract ): DOCS \"\"\"UpSample module generator.\"\"\" def __init__ ( self , * args , ** kwargs ) -> None : super () . __init__ ( * args , ** kwargs ) @property def out_channel ( self ) -> int : return self . in_channel @property def in_channel ( self ) -> int : return self . in_channels [ self . from_idx ] # type: ignore @torch . no_grad () def compute_out_shape ( self , size : np . ndarray , repeat : int = 1 ) -> List [ int ]: module = self ( repeat = repeat ) module_out = module ( torch . zeros ([ 1 , * list ( size )])) return list ( module_out . shape [ 1 :]) def __call__ ( self , repeat : int = 1 ) -> nn . Module : DOCS return self . _get_module ( nn . Upsample ( scale_factor = 2 ))","title":"kindle.generator.upsample"},{"location":"api/source/kindle/","text":"SOURCE CODE kindle DOCS \"\"\"Kindle is an easy model build package for PyTorch. Building a deep learning model became so simple that almost all model can be made by copy and paste from other existing model codes. So why code? when we can simply build a model with yaml markup file. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com - URL: https://github.com/JeiKeiLim/kindle \"\"\" from kindle import generator , modules from kindle.model import Model from kindle.trainer import TorchTrainer __all__ = [ \"modules\" , \"generator\" , \"Model\" , \"TorchTrainer\" ]","title":"kindle"},{"location":"api/source/kindle.model/","text":"SOURCE CODE kindle. model DOCS \"\"\"Kindle Model parser and model. This module parses model configuration yaml file and generates PyTorch model accordingly. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" import os from typing import Any , Dict , List , Optional , Tuple , Type , Union import numpy as np import torch import torch.nn as nn import yaml from kindle.generator.base_generator import GeneratorAbstract , ModuleGenerator from kindle.generator.flatten import FlattenGenerator class Model ( nn . Module ): DOCS \"\"\"PyTorch model class.\"\"\" def __init__ ( self , cfg : Union [ str , Dict [ str , Type ]], verbose : bool = False , ) -> None : \"\"\"Parse model from the model config file. Args: cfg: yaml file path or dictionary type of the model. verbose: print the model parsing information. \"\"\" super () . __init__ () self . model_parser = ModelParser ( cfg = cfg , verbose = verbose ) self . model = self . model_parser . model self . output_save = self . model_parser . output_save def forward ( self , x : torch . Tensor ) -> torch . Tensor : DOCS \"\"\"Forward the model. For the time being, this method will only call self.forward_once. Later, we plan to add Test Time Augment. \"\"\" return self . forward_once ( x ) def forward_once ( self , x : torch . Tensor ) -> torch . Tensor : DOCS \"\"\"Forward one time only.\"\"\" y : List [ Union [ torch . Tensor , None ]] = [] for module in self . model : # type: ignore if module . from_idx != - 1 : x = ( y [ module . from_idx ] # type: ignore if isinstance ( module . from_idx , int ) else [ x if j == - 1 else y [ j ] for j in module . from_idx ] ) x = module ( x ) y . append ( x if module . module_idx in self . output_save else None ) return x class ModelParser : DOCS \"\"\"Generate PyTorch model from the model yaml file.\"\"\" def __init__ ( self , cfg : Union [ str , Dict [ str , Type ]] = \"./model_configs/show_case.yaml\" , verbose : bool = False , ) -> None : \"\"\"Generate PyTorch model from the model yaml file. Args: cfg: model config file or dict values read from the model config file. verbose: print the parsed model information. \"\"\" self . verbose = verbose if isinstance ( cfg , dict ): self . cfg = cfg else : with open ( cfg ) as f : self . cfg = yaml . load ( f , Loader = yaml . FullLoader ) self . input_size = None if ( self . cfg and \"input_size\" in self . cfg and len ( self . cfg [ \"input_size\" ]) == 2 # type: ignore ): self . input_size = self . cfg [ \"input_size\" ] self . custom_module_paths : Optional [ Union [ List [ str ], str ]] if \"custom_module_paths\" in self . cfg : self . custom_module_paths = self . cfg [ \"custom_module_paths\" ] # type: ignore else : self . custom_module_paths = None self . in_channel = self . cfg [ \"input_channel\" ] self . depth_multiply = self . cfg [ \"depth_multiple\" ] self . width_multiply = self . cfg [ \"width_multiple\" ] self . backbone_cfg : List [ Union [ int , str , float ]] = self . cfg [ \"backbone\" ] # type: ignore if \"head\" in self . cfg : self . head_cfg : Optional [ List [ Union [ int , str , float ]]] = self . cfg [ \"head\" ] # type: ignore else : self . head_cfg = None self . model , self . output_save = self . _parse_model () def log ( self , msg : str ): DOCS \"\"\"Log.\"\"\" if self . verbose : print ( msg ) def _log_parse ( self , info : Optional [ Tuple [ int , int , int ]] = None , module : Optional [ nn . Module ] = None , module_generator : Optional [ GeneratorAbstract ] = None , args : Optional [ List [ Any ]] = None , in_size : Optional [ Union [ np . ndarray , List ]] = None , out_size : Optional [ List [ int ]] = None , head : bool = False , ) -> None : \"\"\"Print log message for parsing modules. Args: info: (i, idx, repeat) Current parsing information. module: Parsed module. module_generator: Module generator used to generate module. args: Arguments of the module. in_size: Input size of the module. Only required when {self.input_size} is not None. out_size: Output size of the module Only required when {self.input_size} is not None. head: Print head(Column names) message only. \"\"\" if head : log = ( f \" { 'idx' : >3 } | { 'from' : >10 } | { 'n' : >3 } | { 'params' : >8 } |\" f \" { 'module' : >15 } | { 'arguments' : >20 } |\" f \" { 'in_channel' : >10 } | { 'out_channel' : >11 } |\" ) if self . input_size is not None : log += f \" { 'in shape' : >30 } | { 'out shape' : >15 } |\" log += f \" \\n { len ( log ) * '-' } \" else : assert ( info is not None and module is not None and module_generator is not None and args is not None ), \"info, module, and module_generator must be provided to generate log string.\" i , idx , repeat = info args = args . copy () if module . type == \"YamlModule\" : args [ 0 ] = args [ 0 ] . split ( os . sep )[ - 1 ] . split ( \".\" )[ 0 ] args_str = str ( args ) args_str_list = [] for j in range ( 0 , len ( args_str ), 20 ): end_idx = j + 20 args_str_list . append ( args_str [ j : end_idx ]) args_str = args_str_list [ 0 ] log = ( f \" { i : 3d } | { str ( idx ) : >10 } | { repeat : 3d } |\" f \" { module . n_params : 8,d } | { module . type : >15 } | { args_str : >20 } |\" f \" { module_generator . in_channel : >10 } | { module_generator . out_channel : >11 } |\" ) for j in range ( 1 , len ( args_str_list )): log += ( f \" \\n { '' : >3 } | { '' : >10 } | { '' : >3 } |\" f \" { '' : >8 } | { '' : >15 } | { args_str_list [ j ] : >20 } |\" f \" { '' : >10 } | { '' : >11 } |\" ) if ( self . input_size is not None and in_size is not None and out_size is not None ): in_size_str = str ( in_size ) . replace ( \" \\n \" , \",\" ) log += f \" { in_size_str : >30 } | { str ( out_size ) : >15 } |\" self . log ( log ) def _parse_model ( # pylint: disable=too-many-locals self , ) -> Tuple [ nn . Sequential , List [ int ]]: \"\"\"Parse model.\"\"\" in_channels : List [ int ] = [] in_sizes : List [ int ] = [] layers : List [ nn . Module ] = [] output_save : List [ int ] = [] self . _log_parse ( head = True ) if self . head_cfg is not None : model_cfg = self . backbone_cfg + self . head_cfg else : model_cfg = self . backbone_cfg channel_divisor = GeneratorAbstract . CHANNEL_DIVISOR for i , ( idx , repeat , module , args ) in enumerate ( model_cfg ): # type: ignore if i >= len ( self . backbone_cfg ): GeneratorAbstract . CHANNEL_DIVISOR = 1 width_multiply = 1.0 depth_multiply = 1.0 else : width_multiply = float ( self . width_multiply ) # type: ignore depth_multiply = float ( self . depth_multiply ) # type: ignore module_generator = ModuleGenerator ( module , custom_module_paths = self . custom_module_paths )( * args , from_idx = idx , in_channels = tuple ( in_channels ) if i > 0 else ( self . in_channel ,), # type: ignore width_multiply = width_multiply , ) repeat = max ( round ( repeat * depth_multiply ), 1 ) if repeat > 1 else repeat if isinstance ( module_generator , FlattenGenerator ): if self . input_size is not None : module_generator . args = in_sizes [ idx ] # type: ignore else : module_generator . args = [ in_channels [ idx ]] # type: ignore module = module_generator ( repeat = repeat ) module . module_idx , module . from_idx = i , idx if self . input_size is not None : in_size = ( np . array ( in_sizes , dtype = np . object_ )[ idx ] if i > 0 else [ self . in_channel ] + self . input_size ) out_size = module_generator . compute_out_shape ( in_size , repeat = repeat ) in_sizes . append ( out_size ) else : in_size , out_size = None , None in_channels . append ( module_generator . out_channel ) layers . append ( module ) output_save . extend ( [ x % i for x in ([ idx ] if isinstance ( idx , int ) else idx ) if x != - 1 ] ) self . _log_parse ( info = ( i , idx , repeat ), module = module , module_generator = module_generator , args = args , in_size = in_size , out_size = out_size , ) GeneratorAbstract . CHANNEL_DIVISOR = channel_divisor parsed_model = nn . Sequential ( * layers ) n_param = sum ([ x . numel () for x in parsed_model . parameters ()]) n_grad = sum ([ x . numel () for x in parsed_model . parameters () if x . requires_grad ]) self . log ( f \"Model Summary: { len ( list ( parsed_model . modules ())) : ,d } \" f \"layers, { n_param : ,d } parameters, { n_grad : ,d } gradients\" f \" \\n \" ) return parsed_model , output_save","title":"kindle.model"},{"location":"api/source/kindle.modules.add/","text":"SOURCE CODE kindle.modules. add DOCS \"\"\"Module Description. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" from typing import List , Tuple , Union import torch from torch import nn class Add ( nn . Module ): DOCS \"\"\"Add module for Kindle.\"\"\" def __init_ ( self ): \"\"\"Initialize module.\"\"\" super () . __init__ () @classmethod DOCS def forward ( cls , x : Union [ Tuple [ torch . Tensor , ... ], List [ torch . Tensor ]] ) -> torch . Tensor : \"\"\"Add inputs. Args: x: list of torch tensors Returns: sum of all x's \"\"\" result = x [ 0 ] for i in range ( 1 , len ( x )): result = result + x [ i ] return result","title":"kindle.modules.add"},{"location":"api/source/kindle.modules.bottleneck/","text":"SOURCE CODE kindle.modules. bottleneck DOCS \"\"\"Bottleneck(ResNet) module. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" from typing import Union import torch from torch import nn from kindle.modules.conv import Conv class Bottleneck ( nn . Module ): DOCS \"\"\"Standard bottleneck block. Arguments: [channel, shortcut, groups, expansion, activation] \"\"\" def __init__ ( self , in_channels : int , out_channels : int , shortcut = True , groups : int = 1 , expansion : float = 0.5 , activation : Union [ str , None ] = \"ReLU\" , ) -> None : \"\"\"Initialize.\"\"\" super () . __init__ () expansion_channel = int ( out_channels * expansion ) self . conv1 = Conv ( in_channels , expansion_channel , 1 , 1 , activation = activation ) self . conv2 = Conv ( expansion_channel , out_channels , 3 , 1 , groups = groups ) self . shortcut = shortcut and in_channels == out_channels def forward ( self , x : torch . Tensor ) -> torch . Tensor : DOCS \"\"\"Forward.\"\"\" out = self . conv2 ( self . conv1 ( x )) if self . shortcut : out = out + x return out","title":"kindle.modules.bottleneck"},{"location":"api/source/kindle.modules.concat/","text":"SOURCE CODE kindle.modules. concat DOCS \"\"\"Concat module. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" # pylint: disable=useless-super-delegation from typing import List , Tuple , Union import torch from torch import nn class Concat ( nn . Module ): DOCS \"\"\"Concatenation module. Arguments: [dimension] \"\"\" def __init__ ( self , dimension : int = 1 ) -> None : \"\"\"Concatenation module. Args: dimension: concatenation axis. \"\"\" super () . __init__ () self . dimension = dimension def forward ( self , x : Union [ Tuple [ torch . Tensor , ... ], List [ torch . Tensor ]]): DOCS \"\"\"Forward.\"\"\" return torch . cat ( x , self . dimension )","title":"kindle.modules.concat"},{"location":"api/source/kindle.modules.conv/","text":"SOURCE CODE kindle.modules. conv DOCS \"\"\"Conv module. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" from typing import Union import torch from torch import nn from kindle.torch_utils import Activation , autopad class Conv ( nn . Module ): DOCS \"\"\"Standard convolution with batch normalization and activation. Arguments: [channel, kernel_size, stride, padding, groups, activation] \"\"\" def __init__ ( self , in_channels : int , out_channels : int , kernel_size : int , stride : int = 1 , padding : Union [ int , None ] = None , groups : int = 1 , activation : Union [ str , None ] = \"ReLU\" , ) -> None : \"\"\"Standard convolution with batch normalization and activation. Args: in_channels: input channels. out_channels: output channels. kernel_size: kernel size. stride: stride. padding: input padding. If None is given, autopad is applied which is identical to padding='SAME' in TensorFlow. groups: group convolution. activation: activation name. If None is given, nn.Identity is applied which is no activation. \"\"\" super () . __init__ () # error: Argument \"padding\" to \"Conv2d\" has incompatible type \"Union[int, List[int]]\"; # expected \"Union[int, Tuple[int, int]]\" self . conv = nn . Conv2d ( in_channels , out_channels , kernel_size , stride , padding = autopad ( kernel_size , padding ), # type: ignore groups = groups , bias = False , ) self . batch_norm = nn . BatchNorm2d ( out_channels ) self . activation = Activation ( activation )() def forward ( self , x : torch . Tensor ) -> torch . Tensor : DOCS \"\"\"Forward.\"\"\" return self . activation ( self . batch_norm ( self . conv ( x ))) def fusefoward ( self , x : torch . Tensor ) -> torch . Tensor : DOCS \"\"\"Fuse forward.\"\"\" return self . activation ( self . conv ( x ))","title":"kindle.modules.conv"},{"location":"api/source/kindle.modules.dwconv/","text":"SOURCE CODE kindle.modules. dwconv DOCS \"\"\"DWConv module. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" import math # pylint: disable=useless-super-delegation from typing import Union import torch from torch import nn from kindle.torch_utils import Activation , autopad class DWConv ( nn . Module ): DOCS \"\"\"Depthwise convolution with batch normalization and activation. Arguments: [channel, kernel_size, stride, padding, activation] \"\"\" def __init__ ( self , in_channels : int , out_channels : int , kernel_size : int , stride : int = 1 , padding : Union [ int , None ] = None , activation : Union [ str , None ] = \"ReLU\" , ) -> None : \"\"\"Depthwise convolution with batch normalization and activation. Args: in_channels: input channels. out_channels: output channels. kernel_size: kernel size. stride: stride. padding: input padding. If None is given, autopad is applied which is identical to padding='SAME' in TensorFlow. activation: activation name. If None is given, nn.Identity is applied which is no activation. \"\"\" super () . __init__ () # error: Argument \"padding\" to \"Conv2d\" has incompatible type \"Union[int, List[int]]\"; # expected \"Union[int, Tuple[int, int]]\" self . conv = nn . Conv2d ( in_channels , out_channels , kernel_size , stride , padding = autopad ( kernel_size , padding ), # type: ignore groups = math . gcd ( in_channels , out_channels ), bias = False , ) self . batch_norm = nn . BatchNorm2d ( out_channels ) self . activation = Activation ( activation )() def forward ( self , x : torch . Tensor ) -> torch . Tensor : DOCS \"\"\"Forward.\"\"\" return self . activation ( self . batch_norm ( self . conv ( x ))) def fusefoward ( self , x : torch . Tensor ) -> torch . Tensor : DOCS \"\"\"Fuse forward.\"\"\" return self . activation ( self . conv ( x ))","title":"kindle.modules.dwconv"},{"location":"api/source/kindle.modules.linear/","text":"SOURCE CODE kindle.modules. linear DOCS \"\"\"Linear module. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" from typing import Union import torch from torch import nn from kindle.torch_utils import Activation class Linear ( nn . Module ): DOCS \"\"\"Linear module. Arguments: [channel, activation] \"\"\" def __init__ ( self , in_channel : int , out_channel : int , activation : Union [ str , None ]): \"\"\" Args: in_channels: input channels. out_channels: output channels. activation: activation name. If None is given, nn.Identity is applied which is no activation. \"\"\" super () . __init__ () self . linear = nn . Linear ( in_channel , out_channel ) self . activation = Activation ( activation )() def forward ( self , x : torch . Tensor ) -> torch . Tensor : DOCS \"\"\"Forward.\"\"\" return self . activation ( self . linear ( x ))","title":"kindle.modules.linear"},{"location":"api/source/kindle.modules/","text":"SOURCE CODE kindle. modules DOCS \"\"\"PyTorch Modules.\"\"\" from kindle.modules.bottleneck import Bottleneck from kindle.modules.concat import Concat from kindle.modules.conv import Conv from kindle.modules.dwconv import DWConv from kindle.modules.linear import Linear from kindle.modules.poolings import GlobalAvgPool __all__ = [ \"Bottleneck\" , \"Concat\" , \"Conv\" , \"DWConv\" , \"Linear\" , \"GlobalAvgPool\" , ]","title":"kindle.modules"},{"location":"api/source/kindle.modules.poolings/","text":"SOURCE CODE kindle.modules. poolings DOCS \"\"\"Module generator related to pooling operations. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" from torch import nn class GlobalAvgPool ( nn . AdaptiveAvgPool2d ): DOCS \"\"\"Global average pooling module. Arguments: [] \"\"\" def __init__ ( self ): \"\"\"Initialize.\"\"\" super () . __init__ ( output_size = 1 )","title":"kindle.modules.poolings"},{"location":"api/source/kindle.torch_utils/","text":"SOURCE CODE kindle. torch_utils DOCS \"\"\"Common utility functions. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" import ast import math from typing import List , Tuple , Union import numpy as np import torch from torch import nn from torch.utils.data.sampler import SubsetRandomSampler def split_dataset_index ( DOCS n_data : int , split_ratio : float = 0.1 ) -> Tuple [ SubsetRandomSampler , SubsetRandomSampler ]: \"\"\"Split dataset indices with split_ratio. Args: n_data: number of total data split_ratio: split ratio (0.0 ~ 1.0) Returns: SubsetRandomSampler ({split_ratio} ~ 1.0) SubsetRandomSampler (0 ~ {split_ratio}) \"\"\" indices = np . arange ( n_data ) split = int ( split_ratio * indices . shape [ 0 ]) train_idx = indices [ split :] valid_idx = indices [: split ] train_sampler = SubsetRandomSampler ( train_idx ) valid_sampler = SubsetRandomSampler ( valid_idx ) return train_sampler , valid_sampler def model_info ( model , verbose = False ): DOCS \"\"\"Print out model info.\"\"\" n_p = sum ( x . numel () for x in model . parameters ()) # number parameters n_g = sum ( x . numel () for x in model . parameters () if x . requires_grad ) # number gradients if verbose : print ( \" %5s %40s %9s %12s %20s %10s %10s \" % ( \"layer\" , \"name\" , \"gradient\" , \"parameters\" , \"shape\" , \"mu\" , \"sigma\" ) ) for i , ( name , param ) in enumerate ( model . named_parameters ()): name = name . replace ( \"module_list.\" , \"\" ) print ( \" %5g %40s %9s %12g %20s %10.3g %10.3g \" % ( i , name , param . requires_grad , param . numel (), list ( param . shape ), param . mean (), param . std (), ) ) print ( f \"Model Summary: { len ( list ( model . modules ())) } layers, \" f \" { n_p : ,d } parameters, { n_g : ,d } gradients\" ) def make_divisible ( n_channel : Union [ int , float ], divisor : int = 8 ) -> int : DOCS \"\"\"Convert {n_channel} to divisible by {divisor} Args: n_channel: number of channels. divisor: divisor to be used. Returns: Ex) n_channel=22, divisor=8 ceil(22/8) * 8 = 24 \"\"\" return int ( math . ceil ( n_channel / divisor ) * divisor ) def autopad ( DOCS kernel_size : Union [ int , List [ int ]], padding : Union [ int , None ] = None ) -> Union [ int , List [ int ]]: \"\"\"Auto padding calculation for pad='same' in TensorFlow.\"\"\" # Pad to 'same' if isinstance ( kernel_size , int ): kernel_size = [ kernel_size ] return padding or [ x // 2 for x in kernel_size ] class Activation : DOCS \"\"\"Convert string activation name to the activation class.\"\"\" def __init__ ( self , act_type : Union [ str , None ]) -> None : \"\"\"Convert string activation name to the activation class. Args: act_type: Activation name. Returns: nn.Identity if {type} is None. \"\"\" self . type = act_type self . args = [ 1 ] if self . type == \"Softmax\" else [] def __call__ ( self ) -> nn . Module : if self . type is None : return nn . Identity () if hasattr ( nn , self . type ): return getattr ( nn , self . type )( * self . args ) return ast . literal_eval ( self . type )() def count_model_params ( DOCS model : torch . nn . Module , ) -> int : \"\"\"Count model's parameters.\"\"\" return sum ( p . numel () for p in model . parameters () if p . requires_grad )","title":"kindle.torch_utils"},{"location":"api/source/kindle.trainer/","text":"SOURCE CODE kindle. trainer DOCS \"\"\"PyTorch trainer module. - Author: Jongkuk Lim - Contact: lim.jeikei@gmail.com \"\"\" from typing import Optional , Tuple , Union import numpy as np import torch import torch.nn as nn import torch.optim as optim from torch.utils.data.dataloader import DataLoader from torch.utils.data.sampler import SequentialSampler , SubsetRandomSampler from tqdm import tqdm def _get_n_data_from_dataloader ( dataloader : DataLoader ) -> int : \"\"\"Get a number of data in dataloader. Args: dataloader: torch data loader Returns: A number of data in dataloader \"\"\" if isinstance ( dataloader . sampler , SubsetRandomSampler ): n_data = len ( dataloader . sampler . indices ) elif isinstance ( dataloader . sampler , SequentialSampler ): n_data = len ( dataloader . sampler . data_source ) else : n_data = len ( dataloader ) * dataloader . batch_size if dataloader . batch_size else 1 return n_data def _get_n_batch_from_dataloader ( dataloader : DataLoader ) -> int : \"\"\"Get a batch number in dataloader. Args: dataloader: torch data loader Returns: A batch number in dataloader \"\"\" n_data = _get_n_data_from_dataloader ( dataloader ) n_batch = dataloader . batch_size if dataloader . batch_size else 1 return n_data // n_batch class TorchTrainer : DOCS \"\"\"Pytorch Trainer.\"\"\" def __init__ ( self , model : nn . Module , criterion : nn . Module , optimizer : optim . Optimizer , device : Union [ str , torch . device ] = \"cpu\" , verbose : int = 1 , ) -> None : \"\"\"Initialize TorchTrainer class. Args: model: model to train criterion: loss function module optimizer: optimization module device: torch device verbose: verbosity level. \"\"\" if isinstance ( device , str ): device = torch . device ( device ) self . model = model self . criterion = criterion self . optimizer = optimizer self . verbose = verbose self . device = device def train ( DOCS self , train_dataloader : DataLoader , n_epoch : int , shuffle : bool = False , test_dataloader : Optional [ DataLoader ] = None , ) -> Tuple [ float , float ]: \"\"\"Train model. Args: train_dataloader: data loader module which is a iterator that returns (data, labels) n_epoch: number of total epochs for training test_dataloader: test data loader shuffle: shuffle train data on every epoch. Sampler must be SubsetRandomSampler to apply shuffle. Returns: loss and accuracy \"\"\" average_loss , accuracy = - 1.0 , - 1.0 n_batch = _get_n_batch_from_dataloader ( train_dataloader ) for epoch in range ( n_epoch ): if shuffle and isinstance ( train_dataloader . sampler , SubsetRandomSampler ): np . random . shuffle ( train_dataloader . sampler . indices ) running_loss = 0.0 correct = 0 total = 0 pbar = tqdm ( enumerate ( train_dataloader ), total = n_batch ) for batch , ( data , labels ) in pbar : data , labels = data . to ( self . device ), labels . to ( self . device ) self . optimizer . zero_grad () model_out = self . model ( data ) loss = self . criterion ( model_out , labels ) loss . backward () self . optimizer . step () # TODO: Modify for multi-label classification. _ , predicted = torch . max ( model_out , 1 ) total += labels . size ( 0 ) correct += ( predicted == labels ) . sum () . item () running_loss += loss . item () pbar . update () pbar . set_description ( f \"Train: [ { epoch + 1 : 03d } ] \" f \"Loss: { ( running_loss / ( batch + 1 )) : .7f } , \" f \"Accuracy: { ( correct / total ) * 100 : .2f } %\" ) pbar . close () if test_dataloader is not None : self . test ( test_dataloader ) average_loss = running_loss / n_batch accuracy = correct / total return average_loss , accuracy @torch . no_grad () DOCS def test ( self , test_dataloader : DataLoader ) -> Tuple [ float , float ]: \"\"\"Test model. Args: test_dataloader: test data loader module which is a iterator that returns (data, labels) Returns: loss, accuracy \"\"\" n_batch = _get_n_batch_from_dataloader ( test_dataloader ) running_loss = 0.0 correct = 0 total = 0 pbar = tqdm ( enumerate ( test_dataloader ), total = n_batch ) for batch , ( data , labels ) in pbar : data , labels = data . to ( self . device ), labels . to ( self . device ) model_out = self . model ( data ) running_loss += self . criterion ( model_out , labels ) . item () # TODO: Modify for multi-label classification. _ , predicted = torch . max ( model_out , 1 ) total += labels . size ( 0 ) correct += ( predicted == labels ) . sum () . item () pbar . update () pbar . set_description ( f \" Test: { '' : 5 } Loss: { ( running_loss / ( batch + 1 )) : .7f } , \" f \"Accuracy: { ( correct / total ) * 100 : .2f } %\" ) loss = running_loss / n_batch accuracy = correct / total return loss , accuracy","title":"kindle.trainer"}]}