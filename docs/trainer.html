<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>kindle.trainer API documentation</title>
<meta name="description" content="PyTorch trainer module …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>kindle.trainer</code></h1>
</header>
<section id="section-intro">
<p>PyTorch trainer module.</p>
<ul>
<li>Author: Jongkuk Lim</li>
<li>Contact: lim.jeikei@gmail.com</li>
</ul>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;PyTorch trainer module.

- Author: Jongkuk Lim
- Contact: lim.jeikei@gmail.com
&#34;&#34;&#34;

from typing import Optional, Tuple, Union

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data.dataloader import DataLoader
from torch.utils.data.sampler import SequentialSampler, SubsetRandomSampler
from tqdm import tqdm


def _get_n_data_from_dataloader(dataloader: DataLoader) -&gt; int:
    &#34;&#34;&#34;Get a number of data in dataloader.

    Args:
        dataloader: torch data loader

    Returns:
        A number of data in dataloader
    &#34;&#34;&#34;
    if isinstance(dataloader.sampler, SubsetRandomSampler):
        n_data = len(dataloader.sampler.indices)
    elif isinstance(dataloader.sampler, SequentialSampler):
        n_data = len(dataloader.sampler.data_source)
    else:
        n_data = len(dataloader) * dataloader.batch_size if dataloader.batch_size else 1

    return n_data


def _get_n_batch_from_dataloader(dataloader: DataLoader) -&gt; int:
    &#34;&#34;&#34;Get a batch number in dataloader.

    Args:
        dataloader: torch data loader

    Returns:
        A batch number in dataloader
    &#34;&#34;&#34;
    n_data = _get_n_data_from_dataloader(dataloader)
    n_batch = dataloader.batch_size if dataloader.batch_size else 1

    return n_data // n_batch


class TorchTrainer:
    &#34;&#34;&#34;Pytorch Trainer.&#34;&#34;&#34;

    def __init__(
        self,
        model: nn.Module,
        criterion: nn.Module,
        optimizer: optim.Optimizer,
        device: Union[str, torch.device] = &#34;cpu&#34;,
        verbose: int = 1,
    ) -&gt; None:
        &#34;&#34;&#34;Initialize TorchTrainer class.

        Args:
            model: model to train
            criterion: loss function module
            optimizer: optimization module
            device: torch device
            verbose: verbosity level.
        &#34;&#34;&#34;

        if isinstance(device, str):
            device = torch.device(device)

        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.verbose = verbose
        self.device = device

    def train(
        self,
        train_dataloader: DataLoader,
        n_epoch: int,
        shuffle: bool = False,
        test_dataloader: Optional[DataLoader] = None,
    ) -&gt; Tuple[float, float]:
        &#34;&#34;&#34;Train model.

        Args:
            train_dataloader: data loader module which is a iterator that returns (data, labels)
            n_epoch: number of total epochs for training
            test_dataloader: test data loader
            shuffle: shuffle train data on every epoch.
                     Sampler must be SubsetRandomSampler to apply shuffle.

        Returns:
            loss and accuracy
        &#34;&#34;&#34;
        average_loss, accuracy = -1.0, -1.0
        n_batch = _get_n_batch_from_dataloader(train_dataloader)

        for epoch in range(n_epoch):
            if shuffle and isinstance(train_dataloader.sampler, SubsetRandomSampler):
                np.random.shuffle(train_dataloader.sampler.indices)

            running_loss = 0.0
            correct = 0
            total = 0

            pbar = tqdm(enumerate(train_dataloader), total=n_batch)
            for batch, (data, labels) in pbar:
                data, labels = data.to(self.device), labels.to(self.device)
                self.optimizer.zero_grad()

                model_out = self.model(data)
                loss = self.criterion(model_out, labels)
                loss.backward()
                self.optimizer.step()

                # TODO: Modify for multi-label classification.
                _, predicted = torch.max(model_out, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

                running_loss += loss.item()
                pbar.update()
                pbar.set_description(
                    f&#34;Train: [{epoch + 1:03d}] &#34;
                    f&#34;Loss: {(running_loss / (batch + 1)):.7f}, &#34;
                    f&#34;Accuracy: {(correct / total) * 100:.2f}%&#34;
                )
            pbar.close()

            if test_dataloader is not None:
                self.test(test_dataloader)

            average_loss = running_loss / n_batch
            accuracy = correct / total

        return average_loss, accuracy

    @torch.no_grad()
    def test(self, test_dataloader: DataLoader) -&gt; Tuple[float, float]:
        &#34;&#34;&#34;Test model.

        Args:
            test_dataloader: test data loader module which is a iterator that returns (data, labels)

        Returns:
            loss, accuracy
        &#34;&#34;&#34;

        n_batch = _get_n_batch_from_dataloader(test_dataloader)

        running_loss = 0.0
        correct = 0
        total = 0

        pbar = tqdm(enumerate(test_dataloader), total=n_batch)
        for batch, (data, labels) in pbar:
            data, labels = data.to(self.device), labels.to(self.device)
            model_out = self.model(data)
            running_loss += self.criterion(model_out, labels).item()

            # TODO: Modify for multi-label classification.
            _, predicted = torch.max(model_out, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            pbar.update()
            pbar.set_description(
                f&#34; Test: {&#39;&#39;:5} Loss: {(running_loss / (batch + 1)):.7f}, &#34;
                f&#34;Accuracy: {(correct / total) * 100:.2f}%&#34;
            )

        loss = running_loss / n_batch
        accuracy = correct / total
        return loss, accuracy</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="kindle.trainer.TorchTrainer"><code class="flex name class">
<span>class <span class="ident">TorchTrainer</span></span>
<span>(</span><span>model: torch.nn.modules.module.Module, criterion: torch.nn.modules.module.Module, optimizer: torch.optim.optimizer.Optimizer, device: Union[str, torch.device] = 'cpu', verbose: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Pytorch Trainer.</p>
<p>Initialize TorchTrainer class.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong></dt>
<dd>model to train</dd>
<dt><strong><code>criterion</code></strong></dt>
<dd>loss function module</dd>
<dt><strong><code>optimizer</code></strong></dt>
<dd>optimization module</dd>
<dt><strong><code>device</code></strong></dt>
<dd>torch device</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>verbosity level.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TorchTrainer:
    &#34;&#34;&#34;Pytorch Trainer.&#34;&#34;&#34;

    def __init__(
        self,
        model: nn.Module,
        criterion: nn.Module,
        optimizer: optim.Optimizer,
        device: Union[str, torch.device] = &#34;cpu&#34;,
        verbose: int = 1,
    ) -&gt; None:
        &#34;&#34;&#34;Initialize TorchTrainer class.

        Args:
            model: model to train
            criterion: loss function module
            optimizer: optimization module
            device: torch device
            verbose: verbosity level.
        &#34;&#34;&#34;

        if isinstance(device, str):
            device = torch.device(device)

        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.verbose = verbose
        self.device = device

    def train(
        self,
        train_dataloader: DataLoader,
        n_epoch: int,
        shuffle: bool = False,
        test_dataloader: Optional[DataLoader] = None,
    ) -&gt; Tuple[float, float]:
        &#34;&#34;&#34;Train model.

        Args:
            train_dataloader: data loader module which is a iterator that returns (data, labels)
            n_epoch: number of total epochs for training
            test_dataloader: test data loader
            shuffle: shuffle train data on every epoch.
                     Sampler must be SubsetRandomSampler to apply shuffle.

        Returns:
            loss and accuracy
        &#34;&#34;&#34;
        average_loss, accuracy = -1.0, -1.0
        n_batch = _get_n_batch_from_dataloader(train_dataloader)

        for epoch in range(n_epoch):
            if shuffle and isinstance(train_dataloader.sampler, SubsetRandomSampler):
                np.random.shuffle(train_dataloader.sampler.indices)

            running_loss = 0.0
            correct = 0
            total = 0

            pbar = tqdm(enumerate(train_dataloader), total=n_batch)
            for batch, (data, labels) in pbar:
                data, labels = data.to(self.device), labels.to(self.device)
                self.optimizer.zero_grad()

                model_out = self.model(data)
                loss = self.criterion(model_out, labels)
                loss.backward()
                self.optimizer.step()

                # TODO: Modify for multi-label classification.
                _, predicted = torch.max(model_out, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

                running_loss += loss.item()
                pbar.update()
                pbar.set_description(
                    f&#34;Train: [{epoch + 1:03d}] &#34;
                    f&#34;Loss: {(running_loss / (batch + 1)):.7f}, &#34;
                    f&#34;Accuracy: {(correct / total) * 100:.2f}%&#34;
                )
            pbar.close()

            if test_dataloader is not None:
                self.test(test_dataloader)

            average_loss = running_loss / n_batch
            accuracy = correct / total

        return average_loss, accuracy

    @torch.no_grad()
    def test(self, test_dataloader: DataLoader) -&gt; Tuple[float, float]:
        &#34;&#34;&#34;Test model.

        Args:
            test_dataloader: test data loader module which is a iterator that returns (data, labels)

        Returns:
            loss, accuracy
        &#34;&#34;&#34;

        n_batch = _get_n_batch_from_dataloader(test_dataloader)

        running_loss = 0.0
        correct = 0
        total = 0

        pbar = tqdm(enumerate(test_dataloader), total=n_batch)
        for batch, (data, labels) in pbar:
            data, labels = data.to(self.device), labels.to(self.device)
            model_out = self.model(data)
            running_loss += self.criterion(model_out, labels).item()

            # TODO: Modify for multi-label classification.
            _, predicted = torch.max(model_out, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            pbar.update()
            pbar.set_description(
                f&#34; Test: {&#39;&#39;:5} Loss: {(running_loss / (batch + 1)):.7f}, &#34;
                f&#34;Accuracy: {(correct / total) * 100:.2f}%&#34;
            )

        loss = running_loss / n_batch
        accuracy = correct / total
        return loss, accuracy</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="kindle.trainer.TorchTrainer.test"><code class="name flex">
<span>def <span class="ident">test</span></span>(<span>self, test_dataloader: torch.utils.data.dataloader.DataLoader) ‑> Tuple[float, float]</span>
</code></dt>
<dd>
<div class="desc"><p>Test model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>test_dataloader</code></strong></dt>
<dd>test data loader module which is a iterator that returns (data, labels)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>loss, accuracy</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def test(self, test_dataloader: DataLoader) -&gt; Tuple[float, float]:
    &#34;&#34;&#34;Test model.

    Args:
        test_dataloader: test data loader module which is a iterator that returns (data, labels)

    Returns:
        loss, accuracy
    &#34;&#34;&#34;

    n_batch = _get_n_batch_from_dataloader(test_dataloader)

    running_loss = 0.0
    correct = 0
    total = 0

    pbar = tqdm(enumerate(test_dataloader), total=n_batch)
    for batch, (data, labels) in pbar:
        data, labels = data.to(self.device), labels.to(self.device)
        model_out = self.model(data)
        running_loss += self.criterion(model_out, labels).item()

        # TODO: Modify for multi-label classification.
        _, predicted = torch.max(model_out, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        pbar.update()
        pbar.set_description(
            f&#34; Test: {&#39;&#39;:5} Loss: {(running_loss / (batch + 1)):.7f}, &#34;
            f&#34;Accuracy: {(correct / total) * 100:.2f}%&#34;
        )

    loss = running_loss / n_batch
    accuracy = correct / total
    return loss, accuracy</code></pre>
</details>
</dd>
<dt id="kindle.trainer.TorchTrainer.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, train_dataloader: torch.utils.data.dataloader.DataLoader, n_epoch: int, shuffle: bool = False, test_dataloader: Union[torch.utils.data.dataloader.DataLoader, NoneType] = None) ‑> Tuple[float, float]</span>
</code></dt>
<dd>
<div class="desc"><p>Train model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>train_dataloader</code></strong></dt>
<dd>data loader module which is a iterator that returns (data, labels)</dd>
<dt><strong><code>n_epoch</code></strong></dt>
<dd>number of total epochs for training</dd>
<dt><strong><code>test_dataloader</code></strong></dt>
<dd>test data loader</dd>
<dt><strong><code>shuffle</code></strong></dt>
<dd>shuffle train data on every epoch.
Sampler must be SubsetRandomSampler to apply shuffle.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>loss and accuracy</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(
    self,
    train_dataloader: DataLoader,
    n_epoch: int,
    shuffle: bool = False,
    test_dataloader: Optional[DataLoader] = None,
) -&gt; Tuple[float, float]:
    &#34;&#34;&#34;Train model.

    Args:
        train_dataloader: data loader module which is a iterator that returns (data, labels)
        n_epoch: number of total epochs for training
        test_dataloader: test data loader
        shuffle: shuffle train data on every epoch.
                 Sampler must be SubsetRandomSampler to apply shuffle.

    Returns:
        loss and accuracy
    &#34;&#34;&#34;
    average_loss, accuracy = -1.0, -1.0
    n_batch = _get_n_batch_from_dataloader(train_dataloader)

    for epoch in range(n_epoch):
        if shuffle and isinstance(train_dataloader.sampler, SubsetRandomSampler):
            np.random.shuffle(train_dataloader.sampler.indices)

        running_loss = 0.0
        correct = 0
        total = 0

        pbar = tqdm(enumerate(train_dataloader), total=n_batch)
        for batch, (data, labels) in pbar:
            data, labels = data.to(self.device), labels.to(self.device)
            self.optimizer.zero_grad()

            model_out = self.model(data)
            loss = self.criterion(model_out, labels)
            loss.backward()
            self.optimizer.step()

            # TODO: Modify for multi-label classification.
            _, predicted = torch.max(model_out, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            running_loss += loss.item()
            pbar.update()
            pbar.set_description(
                f&#34;Train: [{epoch + 1:03d}] &#34;
                f&#34;Loss: {(running_loss / (batch + 1)):.7f}, &#34;
                f&#34;Accuracy: {(correct / total) * 100:.2f}%&#34;
            )
        pbar.close()

        if test_dataloader is not None:
            self.test(test_dataloader)

        average_loss = running_loss / n_batch
        accuracy = correct / total

    return average_loss, accuracy</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="kindle" href="index.html">kindle</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="kindle.trainer.TorchTrainer" href="#kindle.trainer.TorchTrainer">TorchTrainer</a></code></h4>
<ul class="">
<li><code><a title="kindle.trainer.TorchTrainer.test" href="#kindle.trainer.TorchTrainer.test">test</a></code></li>
<li><code><a title="kindle.trainer.TorchTrainer.train" href="#kindle.trainer.TorchTrainer.train">train</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>